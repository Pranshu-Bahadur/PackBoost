{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pranshu-Bahadur/PackBoost/blob/main/notebooks/numerai_gpu_demo.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PackBoost Numerai GPU Demo\n",
        "\n",
        "Train PackBoost on Numerai v5.0 with CUDA frontier enabled, bucket eras into user-defined\n",
        "groups, and monitor per-round train/validation correlation as well as trees-per-second.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-packboost"
      },
      "outputs": [],
      "source": [
        "#@title Install PackBoost and dependencies\n",
        "REPO_URL = \"https://github.com/Pranshu-Bahadur/PackBoost.git\"  # change if using a fork\n",
        "\n",
        "import subprocess, sys, os\n",
        "\n",
        "if not os.path.exists('PackBoost'):\n",
        "    subprocess.run(['git', 'clone', REPO_URL, 'PackBoost'], check=True)\n",
        "\n",
        "os.chdir('PackBoost')\n",
        "\n",
        "# Build native extensions (enables CUDA frontier when nvcc is present)\n",
        "subprocess.run([sys.executable, 'setup_native.py', 'build_ext', '--inplace'], check=True)\n",
        "\n",
        "# Install in editable mode so notebooks can import packboost\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', '-e', '.'], check=True)\n",
        "\n",
        "print('PackBoost ready.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Numerai data\n",
        "\n",
        "This cell pulls the v5.0 training and validation parquet files. Provide keys if you plan to\n",
        "upload diagnostics later.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download-data"
      },
      "outputs": [],
      "source": [
        "from numerapi import NumerAPI\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "DATA_VERSION = 'v5.0'\n",
        "DATA_DIR = Path('numerai-data')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "napi = NumerAPI()\n",
        "\n",
        "feature_path = DATA_DIR / 'features.json'\n",
        "train_path = DATA_DIR / 'train.parquet'\n",
        "valid_path = DATA_DIR / 'validation.parquet'\n",
        "\n",
        "if not feature_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/features.json\", str(feature_path))\n",
        "if not train_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/train.parquet\", str(train_path))\n",
        "if not valid_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\", str(valid_path))\n",
        "\n",
        "with feature_path.open('r', encoding='utf-8') as fh:\n",
        "    FEATURES = json.load(fh)['feature_sets']['all']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess\n",
        "\n",
        "* Convert targets to float32 and drop rows where the target is NaN.\n",
        "* Bucket consecutive eras into groups of configurable size (default 64).\n",
        "* Leave features as uint8 bins so we can set `prebinned=True`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "preprocess"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "ERA_BUCKET_SIZE = 64  # feel free to tweak\n",
        "\n",
        "train_df = pd.read_parquet(train_path)\n",
        "train_df = train_df.dropna(subset=['target']).reset_index(drop=True)\n",
        "train_df['era'] = train_df['era'].astype(np.int32)\n",
        "train_df['era_bucket'] = (train_df['era'] // ERA_BUCKET_SIZE).astype(np.int32)\n",
        "\n",
        "Xt = train_df[FEATURES].astype(np.uint8).values\n",
        "yt = train_df['target'].astype(np.float32).values\n",
        "Et = train_df['era_bucket'].to_numpy(np.int32)\n",
        "\n",
        "valid_df = pd.read_parquet(valid_path)\n",
        "valid_df = valid_df.dropna(subset=['target']).reset_index(drop=True)\n",
        "valid_df['era'] = valid_df['era'].astype(np.int32)\n",
        "valid_df['era_bucket'] = (valid_df['era'] // ERA_BUCKET_SIZE).astype(np.int32)\n",
        "\n",
        "Xv = valid_df[FEATURES].astype(np.uint8).values\n",
        "Yv = valid_df['target'].astype(np.float32).values\n",
        "Ev = valid_df['era_bucket'].to_numpy(np.int32)\n",
        "\n",
        "del train_df\n",
        "gc.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train PackBoost with CUDA frontier\n",
        "\n",
        "The training loop logs metrics each round (train/validation correlation and trees per second).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train"
      },
      "outputs": [],
      "source": [
        "from packboost.booster import PackBoost\n",
        "from packboost.config import PackBoostConfig\n",
        "from packboost import backends\n",
        "from time import perf_counter\n",
        "import pandas as pd\n",
        "\n",
        "print('CUDA backend available:', backends.cuda_available())\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "config = PackBoostConfig(\n",
        "    pack_size=8,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.05,\n",
        "    lambda_l2=1e-6,\n",
        "    lambda_dro=0.0,\n",
        "    direction_weight=0.0,\n",
        "    min_samples_leaf=20,\n",
        "    max_bins=64,\n",
        "    k_cuts=0,\n",
        "    device=str(DEVICE),\n",
        "    prebinned=True,\n",
        ")\n",
        "\n",
        "round_logs = []\n",
        "\n",
        "def log_round(idx: int, metrics: dict[str, float]) -> None:\n",
        "    print(f\"Round {metrics['round']:>3}: train_corr={metrics['train_corr']:+.4f} \"\n",
        "          f\"valid_corr={metrics.get('valid_corr', float('nan')):+.4f} \"\n",
        "          f\"trees/s={metrics['trees_per_second']:.2f}\")\n",
        "    round_logs.append(metrics)\n",
        "\n",
        "booster = PackBoost(config)\n",
        "start = perf_counter()\n",
        "booster.fit(\n",
        "    Xt,\n",
        "    yt,\n",
        "    Et,\n",
        "    num_rounds=20,\n",
        "    eval_sets=[('valid', Xv, Yv, Ev)],\n",
        "    round_callback=log_round,\n",
        ")\n",
        "elapsed = perf_counter() - start\n",
        "print(f\"Finished training in {elapsed:.2f} seconds\")\n",
        "\n",
        "metrics_df = pd.DataFrame(round_logs)\n",
        "metrics_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot per-round correlation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "if not metrics_df.empty:\n",
        "    ax = metrics_df.plot(x='round', y=['train_corr', 'valid_corr'], marker='o')\n",
        "    ax.set_ylabel('Correlation')\n",
        "    ax.set_title('PackBoost correlation per round')\n",
        "    ax.grid(True)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate validation predictions\n",
        "\n",
        "Predictions are normalised to the [0,1] range expected by Numerai diagnostics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "predict"
      },
      "outputs": [],
      "source": [
        "pred_valid = booster.predict(Xv)\n",
        "\n",
        "pred_norm = pred_valid.copy()\n",
        "pred_norm -= pred_norm.min()\n",
        "if pred_norm.max() > 0:\n",
        "    pred_norm /= pred_norm.max()\n",
        "pred_norm = np.clip(pred_norm * 0.98 + 0.01, 0.0, 1.0)\n",
        "\n",
        "submission = pd.DataFrame({'prediction': pred_norm})\n",
        "submission.to_csv('packboost_predictions.csv', index=False)\n",
        "print('Saved packboost_predictions.csv')\n",
        "\n",
        "gc.collect()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}