{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40pDPuy1pxy6"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pranshu-Bahadur/PackBoost/blob/main/notebooks/numerai_gpu_demo.ipynb)\n"
      ],
      "id": "40pDPuy1pxy6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tov5riA0pxy7"
      },
      "source": [
        "# PackBoost Numerai GPU Demo\n",
        "\n",
        "Train PackBoost on Numerai v5.0 with CUDA frontier enabled, bucket eras into user-defined\n",
        "groups, and monitor per-round train/validation correlation as well as trees-per-second.\n"
      ],
      "id": "Tov5riA0pxy7"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"]=\"1\""
      ],
      "metadata": {
        "id": "-DQ6KkJwx_AW"
      },
      "id": "-DQ6KkJwx_AW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PackBoost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUrjwmgMzNWt",
        "outputId": "dcc0bcbe-68e8-432f-b4e0-4fbdef9147ab"
      },
      "id": "kUrjwmgMzNWt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PackBoost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PACKBOOST_DISABLE_PACK_PREDICT=0 python examples/hull_benchmark.py  --data datasets/train.csv --era-size 1 --holdout-eras 180 --to-signal-mult 400 --lastn 180  --impute ffill --target-col forward_excess --fx-groups '' --des-off --efb-disable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOvqe4nOzQfe",
        "outputId": "1ef6a0ef-e2aa-4824-d8de-fd13ed0d5b83"
      },
      "id": "YOvqe4nOzQfe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: rows=8,990, base features=94, engineered+base=94, unique raw eras=8990, unique bucketed eras=1\n",
            "Era mode: DES-OFF (single era) | Target: market_forward_excess_returns\n",
            "Engineered on groups: []\n",
            "Impute: ffill\n",
            "DES-OFF row split: train=8810, test=180 (holdout_frac=0.02)\n",
            "EFB (PackBoost): DISABLED.\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "\n",
            "Model         Fit (s)   Predict (s)     R^2   EraMeanCorr   EraSharpe   LastNSharpe   HullPenalized\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "PackBoost       1.955        0.031  -0.0089      -0.0811         nan        0.3000        0.1771\n",
            "XGBoost         0.690        0.002  -0.3028      -0.0113         nan        0.2162        0.0251\n",
            "LightGBM        0.895        0.003  -0.1286       0.0882         nan        0.4361        0.1788\n",
            "CatBoost        3.147        0.002  -0.1232      -0.0536         nan       -0.1622       -0.0540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python examples/hull_benchmark.py  --data datasets/train.csv --era-size 180 --holdout-eras 1 --to-signal-mult 1600 --lastn 180  --impute ffill --target-col forward_excess"
      ],
      "metadata": {
        "id": "Y3OmzFcYIc1D",
        "outputId": "c23c4c26-3f71-4f30-9ca8-4fa1c46e5eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Y3OmzFcYIc1D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:512: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[\"_era_bucket\"] = bucketize_era(df[era_col_raw], size=int(args.era_size))\n",
            "Data: rows=8,990, base features=94, engineered+base=376, unique raw eras=8990, unique bucketed eras=50\n",
            "Era mode: bucketed (size=180) | Target: market_forward_excess_returns\n",
            "Engineered on groups: ['M', 'E', 'I', 'P', 'V', 'S', 'MOM', 'D']\n",
            "Impute: ffill\n",
            "EFB (PackBoost): bundled 376 cols into 104 bundles. New PB dims: train=104, test=104\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "\n",
            "Model         Fit (s)   Predict (s)     R^2   EraMeanCorr   EraSharpe   LastNSharpe   HullPenalized\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "PackBoost      17.646        0.041  -0.0037       0.0373         nan       -0.1010       -0.0463\n",
            "XGBoost         1.848        0.002  -0.1595       0.0085         nan        0.8433        0.4192\n",
            "LightGBM        2.586        0.004  -0.1359       0.0590         nan        0.0250       -0.0300\n",
            "CatBoost        8.970        0.003  -0.1128      -0.0006         nan        0.1056       -0.0145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9dHovf3IW5v"
      },
      "id": "i9dHovf3IW5v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P3Ou_wN1t4n",
        "outputId": "4da4e56c-2582-4993-e28a-9cc2edfaca85"
      },
      "id": "8P3Ou_wN1t4n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PACKBOOST_DISABLE_PACK_PREDICT=0 python examples/synthetic_benchmark.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrzNh8y41oTg",
        "outputId": "5893bb07-c407-408e-d964-d9ae09bc701f"
      },
      "id": "KrzNh8y41oTg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model         Fit (s)   Predict (s)   R^2\n",
            "--------------------------------------------\n",
            "PackBoost       5.109        0.049  0.9302\n",
            "XGBoost         0.843        0.004  0.9619\n",
            "LightGBM        0.453        0.008  0.9413\n",
            "CatBoost        1.131        0.002  0.9915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import packboost._backend as B\n",
        "for name in (\"predict_bins_cuda\",\n",
        "             \"predict_pack_cuda\",\n",
        "             \"partition_frontier_cuda\",\n",
        "             \"find_best_splits_batched_cuda\"):\n",
        "    print(name, type(getattr(B, name, None)))\n"
      ],
      "metadata": {
        "id": "vRQ85W1Vs8kK",
        "outputId": "d2ee83b6-aeb9-46de-efb6-6de4723e23be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vRQ85W1Vs8kK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict_bins_cuda <class 'builtin_function_or_method'>\n",
            "predict_pack_cuda <class 'builtin_function_or_method'>\n",
            "partition_frontier_cuda <class 'builtin_function_or_method'>\n",
            "find_best_splits_batched_cuda <class 'builtin_function_or_method'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PackBoost"
      ],
      "metadata": {
        "id": "3F49uVr7QWal",
        "outputId": "0469030e-6ecb-4f50-f4e1-431403084344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3F49uVr7QWal",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PackBoost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf build"
      ],
      "metadata": {
        "id": "8rDfN8QHQdzf"
      },
      "id": "8rDfN8QHQdzf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find packboost -name \"_backend*.so\" -delete"
      ],
      "metadata": {
        "id": "DgD7MzIOQmWz"
      },
      "id": "DgD7MzIOQmWz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_native.py build_ext --inplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lBc4Lzkts-m",
        "outputId": "e38dc897-6737-40bb-e562-3273b72838c6"
      },
      "id": "8lBc4Lzkts-m",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[setup] compiling CUDA backend: /usr/local/cuda/bin/nvcc -std=c++17 -O3 -Xcompiler -fPIC -c /content/PackBoost/packboost/backends/cuda/frontier_cuda.cu -o /content/PackBoost/build/temp/backend_cuda.o -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_80,code=compute_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -I /usr/local/lib/python3.12/dist-packages/pybind11/include -I /usr/local/lib/python3.12/dist-packages/pybind11/include -I /usr/include/python3.12 -I /usr/local/lib/python3.12/dist-packages/numpy/_core/include -I packboost/backends -DPACKBOOST_ENABLE_CUDA=1\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(270)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"lane\"\u001b[0m was declared but never referenced\n",
            "    const int lane = threadIdx.x & (WARP_SIZE-1);\n",
            "              ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(78)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(133)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(270)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"lane\"\u001b[0m was declared but never referenced\n",
            "    const int lane = threadIdx.x & (WARP_SIZE-1);\n",
            "              ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(78)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(133)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(270)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"lane\"\u001b[0m was declared but never referenced\n",
            "    const int lane = threadIdx.x & (WARP_SIZE-1);\n",
            "              ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(78)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(133)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(270)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"lane\"\u001b[0m was declared but never referenced\n",
            "    const int lane = threadIdx.x & (WARP_SIZE-1);\n",
            "              ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(78)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(133)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "running build_ext\n",
            "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/python3.12 -c flagcheck.cpp -o flagcheck.o -std=c++17\n",
            "building 'packboost._backend' extension\n",
            "creating build/temp.linux-x86_64-cpython-312/packboost/backends\n",
            "creating build/temp.linux-x86_64-cpython-312/packboost/backends/cpu\n",
            "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DPACKBOOST_ENABLE_CUDA=1 -Ipackboost/backends -I/usr/local/lib/python3.12/dist-packages/numpy/_core/include -I/usr/local/lib/python3.12/dist-packages/pybind11/include -I/usr/include/python3.12 -c packboost/backends/bindings.cpp -o build/temp.linux-x86_64-cpython-312/packboost/backends/bindings.o -std=c++17 -fvisibility=hidden -g0 -O3 -std=c++17 -fvisibility=hidden -fopenmp\n",
            "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DPACKBOOST_ENABLE_CUDA=1 -Ipackboost/backends -I/usr/local/lib/python3.12/dist-packages/numpy/_core/include -I/usr/local/lib/python3.12/dist-packages/pybind11/include -I/usr/include/python3.12 -c packboost/backends/cpu/frontier_cpu.cpp -o build/temp.linux-x86_64-cpython-312/packboost/backends/cpu/frontier_cpu.o -std=c++17 -fvisibility=hidden -g0 -O3 -std=c++17 -fvisibility=hidden -fopenmp\n",
            "\u001b[01m\u001b[Kpackboost/backends/cpu/frontier_cpu.cpp:\u001b[m\u001b[K In function \u001b[01m\u001b[Kpybind11::tuple {anonymous}::find_best_splits_batched_cpu(pybind11::array_t<signed char, 17>, pybind11::array_t<float, 17>, const pybind11::list&, pybind11::array_t<int, 17>, int, int, const string&, float, float, float, float, int)\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kpackboost/backends/cpu/frontier_cpu.cpp:148:35:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: \u001b[01m\u001b[Kpybind11::ssize_t\u001b[m\u001b[K {aka \u001b[01m\u001b[Klong int\u001b[m\u001b[K} and \u001b[01m\u001b[Kpybind11::size_t\u001b[m\u001b[K {aka \u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  148 |     for (py::ssize_t idx = 0; \u001b[01;35m\u001b[Kidx < nodes_era_rows.size()\u001b[m\u001b[K; ++idx) {\n",
            "      |                               \u001b[01;35m\u001b[K~~~~^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kpackboost/backends/cpu/frontier_cpu.cpp:163:35:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: \u001b[01m\u001b[Kpybind11::ssize_t\u001b[m\u001b[K {aka \u001b[01m\u001b[Klong int\u001b[m\u001b[K} and \u001b[01m\u001b[Kpybind11::size_t\u001b[m\u001b[K {aka \u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  163 |         for (py::ssize_t e = 0; \u001b[01;35m\u001b[Ke < eras.size()\u001b[m\u001b[K; ++e) {\n",
            "      |                                 \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-cpython-312/packboost\n",
            "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-312/packboost/backends/bindings.o build/temp.linux-x86_64-cpython-312/packboost/backends/cpu/frontier_cpu.o /content/PackBoost/build/temp/backend_cuda.o -L/usr/local/cuda-12.5/lib64 -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-rpath,/usr/local/cuda-12.5/lib64 -lcudart -o build/lib.linux-x86_64-cpython-312/packboost/_backend.cpython-312-x86_64-linux-gnu.so -fopenmp\n",
            "copying build/lib.linux-x86_64-cpython-312/packboost/_backend.cpython-312-x86_64-linux-gnu.so -> packboost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybind11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVCGLY3Fp2GJ",
        "outputId": "f9c31a61-6576-43c4-e5ed-97b59fedb48f"
      },
      "id": "RVCGLY3Fp2GJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybind11\n",
            "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/293.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11\n",
            "Successfully installed pybind11-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-packboost",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc763307-1796-4bed-abf7-7f40cc91cfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PackBoost ready.\n"
          ]
        }
      ],
      "source": [
        "#@title Install PackBoost and dependencies\n",
        "REPO_URL = \"https://github.com/Pranshu-Bahadur/PackBoost.git\"  # change if using a fork\n",
        "\n",
        "import subprocess, sys, os\n",
        "\n",
        "if not os.path.exists('PackBoost'):\n",
        "    subprocess.run(['git', 'clone', REPO_URL, 'PackBoost'], check=True)\n",
        "\n",
        "os.chdir('PackBoost')\n",
        "\n",
        "# Build native extensions (enables CUDA frontier when nvcc is present)\n",
        "subprocess.run([sys.executable, 'setup_native.py', 'build_ext', '--inplace'], check=True)\n",
        "\n",
        "# Install in editable mode so notebooks can import packboost\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', '-e', '.'], check=True)\n",
        "\n",
        "print('PackBoost ready.')\n"
      ],
      "id": "install-packboost"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67b4rPc_pxy8"
      },
      "source": [
        "## Download Numerai data\n",
        "\n",
        "This cell pulls the v5.0 training and validation parquet files. Provide keys if you plan to\n",
        "upload diagnostics later.\n"
      ],
      "id": "67b4rPc_pxy8"
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PackBoost"
      ],
      "metadata": {
        "id": "cH8U_ScPabvt",
        "outputId": "30caa8dc-d767-4497-f527-9da7498eb449",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "cH8U_ScPabvt",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PackBoost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numerapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFiYERJ8rBaM",
        "outputId": "9d816c24-723f-45c2-e78b-66880f0552a7"
      },
      "id": "bFiYERJ8rBaM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numerapi in /usr/local/lib/python3.12/dist-packages (2.20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from numerapi) (2.32.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from numerapi) (2025.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from numerapi) (2.9.0.post0)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.12/dist-packages (from numerapi) (4.67.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from numerapi) (8.2.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from numerapi) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.0->numerapi) (2.0.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.0->numerapi) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->numerapi) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->numerapi) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->numerapi) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->numerapi) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->numerapi) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "download-data"
      },
      "outputs": [],
      "source": [
        "from numerapi import NumerAPI\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "DATA_VERSION = 'v5.0'\n",
        "DATA_DIR = Path('numerai-data')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "napi = NumerAPI()\n",
        "\n",
        "feature_path = DATA_DIR / 'features.json'\n",
        "train_path = DATA_DIR / 'train.parquet'\n",
        "valid_path = DATA_DIR / 'validation.parquet'\n",
        "\n",
        "if not feature_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/features.json\", str(feature_path))\n",
        "if not train_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/train.parquet\", str(train_path))\n",
        "if not valid_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\", str(valid_path))\n",
        "\n",
        "with feature_path.open('r', encoding='utf-8') as fh:\n",
        "    FEATURES = json.load(fh)['feature_sets']['all']\n"
      ],
      "id": "download-data"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW4fbP13pxy8"
      },
      "source": [
        "## Preprocess\n",
        "\n",
        "* Convert targets to float32 and drop rows where the target is NaN.\n",
        "* Bucket consecutive eras into groups of configurable size (default 64).\n",
        "* Leave features as int8 bins so we can set `prebinned=True`.\n"
      ],
      "id": "SW4fbP13pxy8"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "preprocess",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77c5fff-cf8a-4878-9847-521be493ff76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "ERA_BUCKET_SIZE = 512  # feel free to tweak\n",
        "\n",
        "train_df = pd.read_parquet(train_path)\n",
        "#train_df = train_df.dropna(subset=['target']).reset_index(drop=True)\n",
        "train_df['era'] = train_df['era'].astype(np.int16)\n",
        "train_df['era_bucket'] = (train_df['era'] // ERA_BUCKET_SIZE).astype(np.int16)\n",
        "\n",
        "Xt = train_df[FEATURES].astype(np.int8).values\n",
        "yt = train_df['target'].astype(np.float32).values\n",
        "Et = train_df['era_bucket'].to_numpy(np.int16)\n",
        "\n",
        "valid_df = pd.read_parquet(valid_path)\n",
        "valid_df = valid_df.dropna(subset=['target'])#.reset_index(drop=True)\n",
        "valid_df['era'] = valid_df['era'].astype(np.int16)\n",
        "valid_df['era_bucket'] = (valid_df['era'] // ERA_BUCKET_SIZE).astype(np.int16)\n",
        "\n",
        "Xv = valid_df[FEATURES].astype(np.int8).values\n",
        "Yv = valid_df['target'].astype(np.float32).values\n",
        "Ev = valid_df['era_bucket'].to_numpy(np.int16)\n",
        "\n",
        "del train_df\n",
        "gc.collect()\n"
      ],
      "id": "preprocess"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8uMLsMZpxy8"
      },
      "source": [
        "## Train PackBoost with CUDA frontier\n",
        "\n",
        "The training loop logs metrics each round (train/validation correlation and trees per second).\n"
      ],
      "id": "Y8uMLsMZpxy8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PACKBOOST_DISABLE_PACK_PREDICT\"] = \"0\"\n",
        "os.environ[\"PACKBOOST_EVAL_EVERY\"] = \"100\"\n"
      ],
      "metadata": {
        "id": "t7OfR_pCloEI"
      },
      "id": "t7OfR_pCloEI",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "train",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5abad7ed-a0a7-4dd2-afc6-99c890d5ea9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA backend available: True\n",
            "Round   1: train_corr=+0.0202 valid_corr=+nan trees/s=22.81\n",
            "Round   2: train_corr=+0.0238 valid_corr=+nan trees/s=64.90\n",
            "Round   3: train_corr=+0.0271 valid_corr=+nan trees/s=56.59\n",
            "Round   4: train_corr=+0.0314 valid_corr=+nan trees/s=52.55\n",
            "Round   5: train_corr=+0.0326 valid_corr=+nan trees/s=54.61\n",
            "Round   6: train_corr=+0.0335 valid_corr=+nan trees/s=61.92\n",
            "Round   7: train_corr=+0.0355 valid_corr=+nan trees/s=57.81\n",
            "Round   8: train_corr=+0.0372 valid_corr=+nan trees/s=68.64\n",
            "Round   9: train_corr=+0.0383 valid_corr=+nan trees/s=59.89\n",
            "Round  10: train_corr=+0.0397 valid_corr=+nan trees/s=54.57\n",
            "Round  11: train_corr=+0.0403 valid_corr=+nan trees/s=65.68\n",
            "Round  12: train_corr=+0.0409 valid_corr=+nan trees/s=58.38\n",
            "Round  13: train_corr=+0.0411 valid_corr=+nan trees/s=67.75\n",
            "Round  14: train_corr=+0.0419 valid_corr=+nan trees/s=59.42\n",
            "Round  15: train_corr=+0.0423 valid_corr=+nan trees/s=69.47\n",
            "Round  16: train_corr=+0.0425 valid_corr=+nan trees/s=52.50\n",
            "Round  17: train_corr=+0.0436 valid_corr=+nan trees/s=43.32\n",
            "Round  18: train_corr=+0.0445 valid_corr=+nan trees/s=62.20\n",
            "Round  19: train_corr=+0.0447 valid_corr=+nan trees/s=58.06\n",
            "Round  20: train_corr=+0.0453 valid_corr=+nan trees/s=60.60\n",
            "Round  21: train_corr=+0.0465 valid_corr=+nan trees/s=52.40\n",
            "Round  22: train_corr=+0.0468 valid_corr=+nan trees/s=56.47\n",
            "Round  23: train_corr=+0.0472 valid_corr=+nan trees/s=52.23\n",
            "Round  24: train_corr=+0.0477 valid_corr=+nan trees/s=51.77\n",
            "Round  25: train_corr=+0.0482 valid_corr=+nan trees/s=57.45\n",
            "Round  26: train_corr=+0.0483 valid_corr=+nan trees/s=61.73\n",
            "Round  27: train_corr=+0.0487 valid_corr=+nan trees/s=47.84\n",
            "Round  28: train_corr=+0.0489 valid_corr=+nan trees/s=50.85\n",
            "Round  29: train_corr=+0.0493 valid_corr=+nan trees/s=69.80\n",
            "Round  30: train_corr=+0.0498 valid_corr=+nan trees/s=43.10\n",
            "Round  31: train_corr=+0.0502 valid_corr=+nan trees/s=64.84\n",
            "Round  32: train_corr=+0.0503 valid_corr=+nan trees/s=63.09\n",
            "Round  33: train_corr=+0.0503 valid_corr=+nan trees/s=61.18\n",
            "Round  34: train_corr=+0.0509 valid_corr=+nan trees/s=43.76\n",
            "Round  35: train_corr=+0.0514 valid_corr=+nan trees/s=53.50\n",
            "Round  36: train_corr=+0.0516 valid_corr=+nan trees/s=62.30\n",
            "Round  37: train_corr=+0.0522 valid_corr=+nan trees/s=56.18\n",
            "Round  38: train_corr=+0.0523 valid_corr=+nan trees/s=60.82\n",
            "Round  39: train_corr=+0.0525 valid_corr=+nan trees/s=66.51\n",
            "Round  40: train_corr=+0.0527 valid_corr=+nan trees/s=61.11\n",
            "Round  41: train_corr=+0.0532 valid_corr=+nan trees/s=67.66\n",
            "Round  42: train_corr=+0.0533 valid_corr=+nan trees/s=51.68\n",
            "Round  43: train_corr=+0.0534 valid_corr=+nan trees/s=57.41\n",
            "Round  44: train_corr=+0.0535 valid_corr=+nan trees/s=44.36\n",
            "Round  45: train_corr=+0.0540 valid_corr=+nan trees/s=52.84\n",
            "Round  46: train_corr=+0.0544 valid_corr=+nan trees/s=49.08\n",
            "Round  47: train_corr=+0.0547 valid_corr=+nan trees/s=59.54\n",
            "Round  48: train_corr=+0.0553 valid_corr=+nan trees/s=56.02\n",
            "Round  49: train_corr=+0.0557 valid_corr=+nan trees/s=50.16\n",
            "Round  50: train_corr=+0.0562 valid_corr=+nan trees/s=53.70\n",
            "Round  51: train_corr=+0.0565 valid_corr=+nan trees/s=41.78\n",
            "Round  52: train_corr=+0.0571 valid_corr=+nan trees/s=52.34\n",
            "Round  53: train_corr=+0.0574 valid_corr=+nan trees/s=48.64\n",
            "Round  54: train_corr=+0.0578 valid_corr=+nan trees/s=59.22\n",
            "Round  55: train_corr=+0.0581 valid_corr=+nan trees/s=66.14\n",
            "Round  56: train_corr=+0.0584 valid_corr=+nan trees/s=66.56\n",
            "Round  57: train_corr=+0.0586 valid_corr=+nan trees/s=64.30\n",
            "Round  58: train_corr=+0.0587 valid_corr=+nan trees/s=49.69\n",
            "Round  59: train_corr=+0.0589 valid_corr=+nan trees/s=62.96\n",
            "Round  60: train_corr=+0.0590 valid_corr=+nan trees/s=48.25\n",
            "Round  61: train_corr=+0.0595 valid_corr=+nan trees/s=68.25\n",
            "Round  62: train_corr=+0.0597 valid_corr=+nan trees/s=71.14\n",
            "Round  63: train_corr=+0.0600 valid_corr=+nan trees/s=73.82\n",
            "Round  64: train_corr=+0.0604 valid_corr=+nan trees/s=53.70\n",
            "Round  65: train_corr=+0.0610 valid_corr=+nan trees/s=44.51\n",
            "Round  66: train_corr=+0.0615 valid_corr=+nan trees/s=53.39\n",
            "Round  67: train_corr=+0.0621 valid_corr=+nan trees/s=39.50\n",
            "Round  68: train_corr=+0.0623 valid_corr=+nan trees/s=67.31\n",
            "Round  69: train_corr=+0.0626 valid_corr=+nan trees/s=47.29\n",
            "Round  70: train_corr=+0.0630 valid_corr=+nan trees/s=60.93\n",
            "Round  71: train_corr=+0.0633 valid_corr=+nan trees/s=46.45\n",
            "Round  72: train_corr=+0.0637 valid_corr=+nan trees/s=44.10\n",
            "Round  73: train_corr=+0.0638 valid_corr=+nan trees/s=53.95\n",
            "Round  74: train_corr=+0.0640 valid_corr=+nan trees/s=49.59\n",
            "Round  75: train_corr=+0.0643 valid_corr=+nan trees/s=61.38\n",
            "Round  76: train_corr=+0.0647 valid_corr=+nan trees/s=64.14\n",
            "Round  77: train_corr=+0.0649 valid_corr=+nan trees/s=56.69\n",
            "Round  78: train_corr=+0.0652 valid_corr=+nan trees/s=51.56\n",
            "Round  79: train_corr=+0.0655 valid_corr=+nan trees/s=50.92\n",
            "Round  80: train_corr=+0.0657 valid_corr=+nan trees/s=59.29\n",
            "Round  81: train_corr=+0.0661 valid_corr=+nan trees/s=51.93\n",
            "Round  82: train_corr=+0.0663 valid_corr=+nan trees/s=49.56\n",
            "Round  83: train_corr=+0.0667 valid_corr=+nan trees/s=50.20\n",
            "Round  84: train_corr=+0.0671 valid_corr=+nan trees/s=48.00\n",
            "Round  85: train_corr=+0.0674 valid_corr=+nan trees/s=41.22\n",
            "Round  86: train_corr=+0.0676 valid_corr=+nan trees/s=50.59\n",
            "Round  87: train_corr=+0.0679 valid_corr=+nan trees/s=51.56\n",
            "Round  88: train_corr=+0.0683 valid_corr=+nan trees/s=57.40\n",
            "Round  89: train_corr=+0.0684 valid_corr=+nan trees/s=67.67\n",
            "Round  90: train_corr=+0.0685 valid_corr=+nan trees/s=64.02\n",
            "Round  91: train_corr=+0.0689 valid_corr=+nan trees/s=48.02\n",
            "Round  92: train_corr=+0.0690 valid_corr=+nan trees/s=49.04\n",
            "Round  93: train_corr=+0.0694 valid_corr=+nan trees/s=55.96\n",
            "Round  94: train_corr=+0.0697 valid_corr=+nan trees/s=48.33\n",
            "Round  95: train_corr=+0.0701 valid_corr=+nan trees/s=44.93\n",
            "Round  96: train_corr=+0.0703 valid_corr=+nan trees/s=56.85\n",
            "Round  97: train_corr=+0.0705 valid_corr=+nan trees/s=71.51\n",
            "Round  98: train_corr=+0.0708 valid_corr=+nan trees/s=57.93\n",
            "Round  99: train_corr=+0.0711 valid_corr=+nan trees/s=56.34\n",
            "Round 100: train_corr=+0.0714 valid_corr=+0.0193 trees/s=6.28\n",
            "Round 101: train_corr=+0.0717 valid_corr=+0.0193 trees/s=69.99\n",
            "Round 102: train_corr=+0.0718 valid_corr=+0.0193 trees/s=51.55\n",
            "Round 103: train_corr=+0.0719 valid_corr=+0.0193 trees/s=61.80\n",
            "Round 104: train_corr=+0.0722 valid_corr=+0.0193 trees/s=47.80\n",
            "Round 105: train_corr=+0.0723 valid_corr=+0.0193 trees/s=59.64\n",
            "Round 106: train_corr=+0.0726 valid_corr=+0.0193 trees/s=49.59\n",
            "Round 107: train_corr=+0.0729 valid_corr=+0.0193 trees/s=45.87\n",
            "Round 108: train_corr=+0.0731 valid_corr=+0.0193 trees/s=48.46\n",
            "Round 109: train_corr=+0.0733 valid_corr=+0.0193 trees/s=56.46\n",
            "Round 110: train_corr=+0.0735 valid_corr=+0.0193 trees/s=58.87\n",
            "Round 111: train_corr=+0.0739 valid_corr=+0.0193 trees/s=44.33\n",
            "Round 112: train_corr=+0.0742 valid_corr=+0.0193 trees/s=57.92\n",
            "Round 113: train_corr=+0.0744 valid_corr=+0.0193 trees/s=59.20\n",
            "Round 114: train_corr=+0.0746 valid_corr=+0.0193 trees/s=52.59\n",
            "Round 115: train_corr=+0.0748 valid_corr=+0.0193 trees/s=71.20\n",
            "Round 116: train_corr=+0.0750 valid_corr=+0.0193 trees/s=50.20\n",
            "Round 117: train_corr=+0.0751 valid_corr=+0.0193 trees/s=60.50\n",
            "Round 118: train_corr=+0.0753 valid_corr=+0.0193 trees/s=71.29\n",
            "Round 119: train_corr=+0.0755 valid_corr=+0.0193 trees/s=68.22\n",
            "Round 120: train_corr=+0.0756 valid_corr=+0.0193 trees/s=59.52\n",
            "Round 121: train_corr=+0.0758 valid_corr=+0.0193 trees/s=56.92\n",
            "Round 122: train_corr=+0.0759 valid_corr=+0.0193 trees/s=64.33\n",
            "Round 123: train_corr=+0.0760 valid_corr=+0.0193 trees/s=51.38\n",
            "Round 124: train_corr=+0.0762 valid_corr=+0.0193 trees/s=59.06\n",
            "Round 125: train_corr=+0.0765 valid_corr=+0.0193 trees/s=62.19\n",
            "Round 126: train_corr=+0.0765 valid_corr=+0.0193 trees/s=65.38\n",
            "Round 127: train_corr=+0.0766 valid_corr=+0.0193 trees/s=58.60\n",
            "Round 128: train_corr=+0.0768 valid_corr=+0.0193 trees/s=48.36\n",
            "Round 129: train_corr=+0.0770 valid_corr=+0.0193 trees/s=59.07\n",
            "Round 130: train_corr=+0.0772 valid_corr=+0.0193 trees/s=56.57\n",
            "Round 131: train_corr=+0.0775 valid_corr=+0.0193 trees/s=59.23\n",
            "Round 132: train_corr=+0.0778 valid_corr=+0.0193 trees/s=61.21\n",
            "Round 133: train_corr=+0.0781 valid_corr=+0.0193 trees/s=40.62\n",
            "Round 134: train_corr=+0.0783 valid_corr=+0.0193 trees/s=43.54\n",
            "Round 135: train_corr=+0.0783 valid_corr=+0.0193 trees/s=66.80\n",
            "Round 136: train_corr=+0.0785 valid_corr=+0.0193 trees/s=56.46\n",
            "Round 137: train_corr=+0.0787 valid_corr=+0.0193 trees/s=59.98\n",
            "Round 138: train_corr=+0.0788 valid_corr=+0.0193 trees/s=61.52\n",
            "Round 139: train_corr=+0.0789 valid_corr=+0.0193 trees/s=74.88\n",
            "Round 140: train_corr=+0.0792 valid_corr=+0.0193 trees/s=45.55\n",
            "Round 141: train_corr=+0.0792 valid_corr=+0.0193 trees/s=64.01\n",
            "Round 142: train_corr=+0.0794 valid_corr=+0.0193 trees/s=69.89\n",
            "Round 143: train_corr=+0.0795 valid_corr=+0.0193 trees/s=65.39\n",
            "Round 144: train_corr=+0.0798 valid_corr=+0.0193 trees/s=44.19\n",
            "Round 145: train_corr=+0.0800 valid_corr=+0.0193 trees/s=56.01\n",
            "Round 146: train_corr=+0.0802 valid_corr=+0.0193 trees/s=53.48\n",
            "Round 147: train_corr=+0.0805 valid_corr=+0.0193 trees/s=45.14\n",
            "Round 148: train_corr=+0.0806 valid_corr=+0.0193 trees/s=66.14\n",
            "Round 149: train_corr=+0.0809 valid_corr=+0.0193 trees/s=52.12\n",
            "Round 150: train_corr=+0.0810 valid_corr=+0.0193 trees/s=53.81\n",
            "Round 151: train_corr=+0.0812 valid_corr=+0.0193 trees/s=53.18\n",
            "Round 152: train_corr=+0.0814 valid_corr=+0.0193 trees/s=62.25\n",
            "Round 153: train_corr=+0.0816 valid_corr=+0.0193 trees/s=56.39\n",
            "Round 154: train_corr=+0.0817 valid_corr=+0.0193 trees/s=58.15\n",
            "Round 155: train_corr=+0.0819 valid_corr=+0.0193 trees/s=66.69\n",
            "Round 156: train_corr=+0.0821 valid_corr=+0.0193 trees/s=41.90\n",
            "Round 157: train_corr=+0.0822 valid_corr=+0.0193 trees/s=49.14\n",
            "Round 158: train_corr=+0.0825 valid_corr=+0.0193 trees/s=65.16\n",
            "Round 159: train_corr=+0.0825 valid_corr=+0.0193 trees/s=68.86\n",
            "Round 160: train_corr=+0.0826 valid_corr=+0.0193 trees/s=60.90\n",
            "Round 161: train_corr=+0.0827 valid_corr=+0.0193 trees/s=55.23\n",
            "Round 162: train_corr=+0.0830 valid_corr=+0.0193 trees/s=40.24\n",
            "Round 163: train_corr=+0.0832 valid_corr=+0.0193 trees/s=55.57\n",
            "Round 164: train_corr=+0.0835 valid_corr=+0.0193 trees/s=59.67\n",
            "Round 165: train_corr=+0.0837 valid_corr=+0.0193 trees/s=54.82\n",
            "Round 166: train_corr=+0.0838 valid_corr=+0.0193 trees/s=45.91\n",
            "Round 167: train_corr=+0.0840 valid_corr=+0.0193 trees/s=46.73\n",
            "Round 168: train_corr=+0.0842 valid_corr=+0.0193 trees/s=49.12\n",
            "Round 169: train_corr=+0.0844 valid_corr=+0.0193 trees/s=49.01\n",
            "Round 170: train_corr=+0.0845 valid_corr=+0.0193 trees/s=68.70\n",
            "Round 171: train_corr=+0.0846 valid_corr=+0.0193 trees/s=53.56\n",
            "Round 172: train_corr=+0.0849 valid_corr=+0.0193 trees/s=63.13\n",
            "Round 173: train_corr=+0.0851 valid_corr=+0.0193 trees/s=70.09\n",
            "Round 174: train_corr=+0.0852 valid_corr=+0.0193 trees/s=59.74\n",
            "Round 175: train_corr=+0.0853 valid_corr=+0.0193 trees/s=43.32\n",
            "Round 176: train_corr=+0.0857 valid_corr=+0.0193 trees/s=40.41\n",
            "Round 177: train_corr=+0.0859 valid_corr=+0.0193 trees/s=56.92\n",
            "Round 178: train_corr=+0.0861 valid_corr=+0.0193 trees/s=49.87\n",
            "Round 179: train_corr=+0.0862 valid_corr=+0.0193 trees/s=44.13\n",
            "Round 180: train_corr=+0.0864 valid_corr=+0.0193 trees/s=55.71\n",
            "Round 181: train_corr=+0.0865 valid_corr=+0.0193 trees/s=55.93\n",
            "Round 182: train_corr=+0.0867 valid_corr=+0.0193 trees/s=63.63\n",
            "Round 183: train_corr=+0.0868 valid_corr=+0.0193 trees/s=55.13\n",
            "Round 184: train_corr=+0.0870 valid_corr=+0.0193 trees/s=52.59\n",
            "Round 185: train_corr=+0.0871 valid_corr=+0.0193 trees/s=52.41\n",
            "Round 186: train_corr=+0.0872 valid_corr=+0.0193 trees/s=53.33\n",
            "Round 187: train_corr=+0.0874 valid_corr=+0.0193 trees/s=55.04\n",
            "Round 188: train_corr=+0.0875 valid_corr=+0.0193 trees/s=45.34\n",
            "Round 189: train_corr=+0.0877 valid_corr=+0.0193 trees/s=45.75\n",
            "Round 190: train_corr=+0.0878 valid_corr=+0.0193 trees/s=71.20\n",
            "Round 191: train_corr=+0.0880 valid_corr=+0.0193 trees/s=47.97\n",
            "Round 192: train_corr=+0.0881 valid_corr=+0.0193 trees/s=56.04\n",
            "Round 193: train_corr=+0.0882 valid_corr=+0.0193 trees/s=54.48\n",
            "Round 194: train_corr=+0.0883 valid_corr=+0.0193 trees/s=65.07\n",
            "Round 195: train_corr=+0.0886 valid_corr=+0.0193 trees/s=42.14\n",
            "Round 196: train_corr=+0.0887 valid_corr=+0.0193 trees/s=68.73\n",
            "Round 197: train_corr=+0.0889 valid_corr=+0.0193 trees/s=62.44\n",
            "Round 198: train_corr=+0.0890 valid_corr=+0.0193 trees/s=57.13\n",
            "Round 199: train_corr=+0.0892 valid_corr=+0.0193 trees/s=47.69\n",
            "Round 200: train_corr=+0.0894 valid_corr=+0.0222 trees/s=3.33\n",
            "Round 201: train_corr=+0.0896 valid_corr=+0.0222 trees/s=66.37\n",
            "Round 202: train_corr=+0.0896 valid_corr=+0.0222 trees/s=64.86\n",
            "Round 203: train_corr=+0.0898 valid_corr=+0.0222 trees/s=49.53\n",
            "Round 204: train_corr=+0.0900 valid_corr=+0.0222 trees/s=49.07\n",
            "Round 205: train_corr=+0.0902 valid_corr=+0.0222 trees/s=53.48\n",
            "Round 206: train_corr=+0.0903 valid_corr=+0.0222 trees/s=53.69\n",
            "Round 207: train_corr=+0.0905 valid_corr=+0.0222 trees/s=44.61\n",
            "Round 208: train_corr=+0.0908 valid_corr=+0.0222 trees/s=38.67\n",
            "Round 209: train_corr=+0.0910 valid_corr=+0.0222 trees/s=46.46\n",
            "Round 210: train_corr=+0.0912 valid_corr=+0.0222 trees/s=51.76\n",
            "Round 211: train_corr=+0.0914 valid_corr=+0.0222 trees/s=46.69\n",
            "Round 212: train_corr=+0.0916 valid_corr=+0.0222 trees/s=53.89\n",
            "Round 213: train_corr=+0.0919 valid_corr=+0.0222 trees/s=45.89\n",
            "Round 214: train_corr=+0.0920 valid_corr=+0.0222 trees/s=45.39\n",
            "Round 215: train_corr=+0.0921 valid_corr=+0.0222 trees/s=58.64\n",
            "Round 216: train_corr=+0.0922 valid_corr=+0.0222 trees/s=49.21\n",
            "Round 217: train_corr=+0.0923 valid_corr=+0.0222 trees/s=70.79\n",
            "Round 218: train_corr=+0.0926 valid_corr=+0.0222 trees/s=44.48\n",
            "Round 219: train_corr=+0.0927 valid_corr=+0.0222 trees/s=47.97\n",
            "Round 220: train_corr=+0.0929 valid_corr=+0.0222 trees/s=45.09\n",
            "Round 221: train_corr=+0.0932 valid_corr=+0.0222 trees/s=35.85\n",
            "Round 222: train_corr=+0.0933 valid_corr=+0.0222 trees/s=44.09\n",
            "Round 223: train_corr=+0.0935 valid_corr=+0.0222 trees/s=49.25\n",
            "Round 224: train_corr=+0.0937 valid_corr=+0.0222 trees/s=52.06\n",
            "Round 225: train_corr=+0.0939 valid_corr=+0.0222 trees/s=58.06\n",
            "Round 226: train_corr=+0.0940 valid_corr=+0.0222 trees/s=50.45\n",
            "Round 227: train_corr=+0.0941 valid_corr=+0.0222 trees/s=64.59\n",
            "Round 228: train_corr=+0.0941 valid_corr=+0.0222 trees/s=61.91\n",
            "Round 229: train_corr=+0.0943 valid_corr=+0.0222 trees/s=66.22\n",
            "Round 230: train_corr=+0.0944 valid_corr=+0.0222 trees/s=62.10\n",
            "Round 231: train_corr=+0.0945 valid_corr=+0.0222 trees/s=53.72\n",
            "Round 232: train_corr=+0.0946 valid_corr=+0.0222 trees/s=53.38\n",
            "Round 233: train_corr=+0.0946 valid_corr=+0.0222 trees/s=66.44\n",
            "Round 234: train_corr=+0.0947 valid_corr=+0.0222 trees/s=68.71\n",
            "Round 235: train_corr=+0.0948 valid_corr=+0.0222 trees/s=70.61\n",
            "Round 236: train_corr=+0.0950 valid_corr=+0.0222 trees/s=55.53\n",
            "Round 237: train_corr=+0.0950 valid_corr=+0.0222 trees/s=49.16\n",
            "Round 238: train_corr=+0.0951 valid_corr=+0.0222 trees/s=56.31\n",
            "Round 239: train_corr=+0.0953 valid_corr=+0.0222 trees/s=44.03\n",
            "Round 240: train_corr=+0.0954 valid_corr=+0.0222 trees/s=61.49\n",
            "Round 241: train_corr=+0.0956 valid_corr=+0.0222 trees/s=54.63\n",
            "Round 242: train_corr=+0.0959 valid_corr=+0.0222 trees/s=42.84\n",
            "Round 243: train_corr=+0.0960 valid_corr=+0.0222 trees/s=60.50\n",
            "Round 244: train_corr=+0.0961 valid_corr=+0.0222 trees/s=60.65\n",
            "Round 245: train_corr=+0.0963 valid_corr=+0.0222 trees/s=46.16\n",
            "Round 246: train_corr=+0.0964 valid_corr=+0.0222 trees/s=57.67\n",
            "Round 247: train_corr=+0.0965 valid_corr=+0.0222 trees/s=47.22\n",
            "Round 248: train_corr=+0.0966 valid_corr=+0.0222 trees/s=67.84\n",
            "Round 249: train_corr=+0.0969 valid_corr=+0.0222 trees/s=39.73\n",
            "Round 250: train_corr=+0.0971 valid_corr=+0.0222 trees/s=47.42\n",
            "Round 251: train_corr=+0.0973 valid_corr=+0.0222 trees/s=54.37\n",
            "Round 252: train_corr=+0.0976 valid_corr=+0.0222 trees/s=55.36\n",
            "Round 253: train_corr=+0.0978 valid_corr=+0.0222 trees/s=57.56\n",
            "Round 254: train_corr=+0.0981 valid_corr=+0.0222 trees/s=45.00\n",
            "Round 255: train_corr=+0.0983 valid_corr=+0.0222 trees/s=61.86\n",
            "Round 256: train_corr=+0.0984 valid_corr=+0.0222 trees/s=50.52\n",
            "Round 257: train_corr=+0.0986 valid_corr=+0.0222 trees/s=51.29\n",
            "Round 258: train_corr=+0.0988 valid_corr=+0.0222 trees/s=61.94\n",
            "Round 259: train_corr=+0.0989 valid_corr=+0.0222 trees/s=43.21\n",
            "Round 260: train_corr=+0.0990 valid_corr=+0.0222 trees/s=61.85\n",
            "Round 261: train_corr=+0.0993 valid_corr=+0.0222 trees/s=35.00\n",
            "Round 262: train_corr=+0.0995 valid_corr=+0.0222 trees/s=72.44\n",
            "Round 263: train_corr=+0.0996 valid_corr=+0.0222 trees/s=49.91\n",
            "Round 264: train_corr=+0.0997 valid_corr=+0.0222 trees/s=60.67\n",
            "Round 265: train_corr=+0.0998 valid_corr=+0.0222 trees/s=48.01\n",
            "Round 266: train_corr=+0.1000 valid_corr=+0.0222 trees/s=44.17\n",
            "Round 267: train_corr=+0.1002 valid_corr=+0.0222 trees/s=51.11\n",
            "Round 268: train_corr=+0.1003 valid_corr=+0.0222 trees/s=45.02\n",
            "Round 269: train_corr=+0.1004 valid_corr=+0.0222 trees/s=50.53\n",
            "Round 270: train_corr=+0.1005 valid_corr=+0.0222 trees/s=62.05\n",
            "Round 271: train_corr=+0.1007 valid_corr=+0.0222 trees/s=43.30\n",
            "Round 272: train_corr=+0.1009 valid_corr=+0.0222 trees/s=40.96\n",
            "Round 273: train_corr=+0.1011 valid_corr=+0.0222 trees/s=46.71\n",
            "Round 274: train_corr=+0.1012 valid_corr=+0.0222 trees/s=54.14\n",
            "Round 275: train_corr=+0.1013 valid_corr=+0.0222 trees/s=67.22\n",
            "Round 276: train_corr=+0.1014 valid_corr=+0.0222 trees/s=51.79\n",
            "Round 277: train_corr=+0.1015 valid_corr=+0.0222 trees/s=56.19\n",
            "Round 278: train_corr=+0.1016 valid_corr=+0.0222 trees/s=57.28\n",
            "Round 279: train_corr=+0.1017 valid_corr=+0.0222 trees/s=62.73\n",
            "Round 280: train_corr=+0.1018 valid_corr=+0.0222 trees/s=47.45\n",
            "Round 281: train_corr=+0.1020 valid_corr=+0.0222 trees/s=57.39\n",
            "Round 282: train_corr=+0.1021 valid_corr=+0.0222 trees/s=57.88\n",
            "Round 283: train_corr=+0.1023 valid_corr=+0.0222 trees/s=50.74\n",
            "Round 284: train_corr=+0.1024 valid_corr=+0.0222 trees/s=58.16\n",
            "Round 285: train_corr=+0.1024 valid_corr=+0.0222 trees/s=56.11\n",
            "Round 286: train_corr=+0.1026 valid_corr=+0.0222 trees/s=44.57\n",
            "Round 287: train_corr=+0.1027 valid_corr=+0.0222 trees/s=56.13\n",
            "Round 288: train_corr=+0.1029 valid_corr=+0.0222 trees/s=60.98\n",
            "Round 289: train_corr=+0.1030 valid_corr=+0.0222 trees/s=59.72\n",
            "Round 290: train_corr=+0.1033 valid_corr=+0.0222 trees/s=56.22\n",
            "Round 291: train_corr=+0.1034 valid_corr=+0.0222 trees/s=48.08\n",
            "Round 292: train_corr=+0.1035 valid_corr=+0.0222 trees/s=49.63\n",
            "Round 293: train_corr=+0.1036 valid_corr=+0.0222 trees/s=54.74\n",
            "Round 294: train_corr=+0.1037 valid_corr=+0.0222 trees/s=52.15\n",
            "Round 295: train_corr=+0.1038 valid_corr=+0.0222 trees/s=49.83\n",
            "Round 296: train_corr=+0.1038 valid_corr=+0.0222 trees/s=61.74\n",
            "Round 297: train_corr=+0.1039 valid_corr=+0.0222 trees/s=48.57\n",
            "Round 298: train_corr=+0.1041 valid_corr=+0.0222 trees/s=47.61\n",
            "Round 299: train_corr=+0.1042 valid_corr=+0.0222 trees/s=43.80\n",
            "Round 300: train_corr=+0.1043 valid_corr=+0.0238 trees/s=2.29\n",
            "Round 301: train_corr=+0.1043 valid_corr=+0.0238 trees/s=64.67\n",
            "Round 302: train_corr=+0.1044 valid_corr=+0.0238 trees/s=45.17\n",
            "Round 303: train_corr=+0.1045 valid_corr=+0.0238 trees/s=53.04\n",
            "Round 304: train_corr=+0.1048 valid_corr=+0.0238 trees/s=53.49\n",
            "Round 305: train_corr=+0.1049 valid_corr=+0.0238 trees/s=39.14\n",
            "Round 306: train_corr=+0.1051 valid_corr=+0.0238 trees/s=51.94\n",
            "Round 307: train_corr=+0.1053 valid_corr=+0.0238 trees/s=59.69\n",
            "Round 308: train_corr=+0.1056 valid_corr=+0.0238 trees/s=32.52\n",
            "Round 309: train_corr=+0.1057 valid_corr=+0.0238 trees/s=48.64\n",
            "Round 310: train_corr=+0.1059 valid_corr=+0.0238 trees/s=56.53\n",
            "Round 311: train_corr=+0.1060 valid_corr=+0.0238 trees/s=56.22\n",
            "Round 312: train_corr=+0.1061 valid_corr=+0.0238 trees/s=47.66\n",
            "Round 313: train_corr=+0.1062 valid_corr=+0.0238 trees/s=60.79\n",
            "Round 314: train_corr=+0.1063 valid_corr=+0.0238 trees/s=53.23\n",
            "Round 315: train_corr=+0.1064 valid_corr=+0.0238 trees/s=51.49\n",
            "Round 316: train_corr=+0.1065 valid_corr=+0.0238 trees/s=67.52\n",
            "Round 317: train_corr=+0.1066 valid_corr=+0.0238 trees/s=64.21\n",
            "Round 318: train_corr=+0.1068 valid_corr=+0.0238 trees/s=57.99\n",
            "Round 319: train_corr=+0.1069 valid_corr=+0.0238 trees/s=53.20\n",
            "Round 320: train_corr=+0.1069 valid_corr=+0.0238 trees/s=55.65\n",
            "Round 321: train_corr=+0.1071 valid_corr=+0.0238 trees/s=46.50\n",
            "Round 322: train_corr=+0.1071 valid_corr=+0.0238 trees/s=57.91\n",
            "Round 323: train_corr=+0.1073 valid_corr=+0.0238 trees/s=48.72\n",
            "Round 324: train_corr=+0.1075 valid_corr=+0.0238 trees/s=47.20\n",
            "Round 325: train_corr=+0.1076 valid_corr=+0.0238 trees/s=46.19\n",
            "Round 326: train_corr=+0.1077 valid_corr=+0.0238 trees/s=57.30\n",
            "Round 327: train_corr=+0.1078 valid_corr=+0.0238 trees/s=61.16\n",
            "Round 328: train_corr=+0.1080 valid_corr=+0.0238 trees/s=44.80\n",
            "Round 329: train_corr=+0.1081 valid_corr=+0.0238 trees/s=38.36\n",
            "Round 330: train_corr=+0.1083 valid_corr=+0.0238 trees/s=46.15\n",
            "Round 331: train_corr=+0.1084 valid_corr=+0.0238 trees/s=45.39\n",
            "Round 332: train_corr=+0.1085 valid_corr=+0.0238 trees/s=44.82\n",
            "Round 333: train_corr=+0.1086 valid_corr=+0.0238 trees/s=62.88\n",
            "Round 334: train_corr=+0.1088 valid_corr=+0.0238 trees/s=60.77\n",
            "Round 335: train_corr=+0.1089 valid_corr=+0.0238 trees/s=52.95\n",
            "Round 336: train_corr=+0.1090 valid_corr=+0.0238 trees/s=56.73\n",
            "Round 337: train_corr=+0.1090 valid_corr=+0.0238 trees/s=57.40\n",
            "Round 338: train_corr=+0.1091 valid_corr=+0.0238 trees/s=52.69\n",
            "Round 339: train_corr=+0.1094 valid_corr=+0.0238 trees/s=39.81\n",
            "Round 340: train_corr=+0.1095 valid_corr=+0.0238 trees/s=57.97\n",
            "Round 341: train_corr=+0.1097 valid_corr=+0.0238 trees/s=54.74\n",
            "Round 342: train_corr=+0.1099 valid_corr=+0.0238 trees/s=59.25\n",
            "Round 343: train_corr=+0.1099 valid_corr=+0.0238 trees/s=52.88\n",
            "Round 344: train_corr=+0.1101 valid_corr=+0.0238 trees/s=58.14\n",
            "Round 345: train_corr=+0.1103 valid_corr=+0.0238 trees/s=51.28\n",
            "Round 346: train_corr=+0.1104 valid_corr=+0.0238 trees/s=41.70\n",
            "Round 347: train_corr=+0.1105 valid_corr=+0.0238 trees/s=44.68\n",
            "Round 348: train_corr=+0.1105 valid_corr=+0.0238 trees/s=68.80\n",
            "Round 349: train_corr=+0.1106 valid_corr=+0.0238 trees/s=54.96\n",
            "Round 350: train_corr=+0.1108 valid_corr=+0.0238 trees/s=51.85\n",
            "Round 351: train_corr=+0.1109 valid_corr=+0.0238 trees/s=41.11\n",
            "Round 352: train_corr=+0.1110 valid_corr=+0.0238 trees/s=49.92\n",
            "Round 353: train_corr=+0.1112 valid_corr=+0.0238 trees/s=62.28\n",
            "Round 354: train_corr=+0.1113 valid_corr=+0.0238 trees/s=55.45\n",
            "Round 355: train_corr=+0.1115 valid_corr=+0.0238 trees/s=47.10\n",
            "Round 356: train_corr=+0.1116 valid_corr=+0.0238 trees/s=52.24\n",
            "Round 357: train_corr=+0.1118 valid_corr=+0.0238 trees/s=44.79\n",
            "Round 358: train_corr=+0.1119 valid_corr=+0.0238 trees/s=64.05\n",
            "Round 359: train_corr=+0.1120 valid_corr=+0.0238 trees/s=50.44\n",
            "Round 360: train_corr=+0.1121 valid_corr=+0.0238 trees/s=58.99\n",
            "Round 361: train_corr=+0.1122 valid_corr=+0.0238 trees/s=58.31\n",
            "Round 362: train_corr=+0.1122 valid_corr=+0.0238 trees/s=72.12\n",
            "Round 363: train_corr=+0.1122 valid_corr=+0.0238 trees/s=62.73\n",
            "Round 364: train_corr=+0.1123 valid_corr=+0.0238 trees/s=56.21\n",
            "Round 365: train_corr=+0.1124 valid_corr=+0.0238 trees/s=58.60\n",
            "Round 366: train_corr=+0.1125 valid_corr=+0.0238 trees/s=63.01\n",
            "Round 367: train_corr=+0.1126 valid_corr=+0.0238 trees/s=59.61\n",
            "Round 368: train_corr=+0.1128 valid_corr=+0.0238 trees/s=42.58\n",
            "Round 369: train_corr=+0.1129 valid_corr=+0.0238 trees/s=63.99\n",
            "Round 370: train_corr=+0.1129 valid_corr=+0.0238 trees/s=64.93\n",
            "Round 371: train_corr=+0.1130 valid_corr=+0.0238 trees/s=52.59\n",
            "Round 372: train_corr=+0.1131 valid_corr=+0.0238 trees/s=46.37\n",
            "Round 373: train_corr=+0.1132 valid_corr=+0.0238 trees/s=57.98\n",
            "Round 374: train_corr=+0.1132 valid_corr=+0.0238 trees/s=42.81\n",
            "Round 375: train_corr=+0.1133 valid_corr=+0.0238 trees/s=62.31\n",
            "Round 376: train_corr=+0.1134 valid_corr=+0.0238 trees/s=62.34\n",
            "Round 377: train_corr=+0.1135 valid_corr=+0.0238 trees/s=59.58\n",
            "Round 378: train_corr=+0.1136 valid_corr=+0.0238 trees/s=56.38\n",
            "Round 379: train_corr=+0.1138 valid_corr=+0.0238 trees/s=49.89\n",
            "Round 380: train_corr=+0.1139 valid_corr=+0.0238 trees/s=55.02\n",
            "Round 381: train_corr=+0.1140 valid_corr=+0.0238 trees/s=66.31\n",
            "Round 382: train_corr=+0.1141 valid_corr=+0.0238 trees/s=65.57\n",
            "Round 383: train_corr=+0.1143 valid_corr=+0.0238 trees/s=19.63\n",
            "Round 384: train_corr=+0.1144 valid_corr=+0.0238 trees/s=47.08\n",
            "Round 385: train_corr=+0.1144 valid_corr=+0.0238 trees/s=54.53\n",
            "Round 386: train_corr=+0.1144 valid_corr=+0.0238 trees/s=63.12\n",
            "Round 387: train_corr=+0.1146 valid_corr=+0.0238 trees/s=55.19\n",
            "Round 388: train_corr=+0.1147 valid_corr=+0.0238 trees/s=52.95\n",
            "Round 389: train_corr=+0.1148 valid_corr=+0.0238 trees/s=57.31\n",
            "Round 390: train_corr=+0.1149 valid_corr=+0.0238 trees/s=53.38\n",
            "Round 391: train_corr=+0.1151 valid_corr=+0.0238 trees/s=50.02\n",
            "Round 392: train_corr=+0.1152 valid_corr=+0.0238 trees/s=38.81\n",
            "Round 393: train_corr=+0.1152 valid_corr=+0.0238 trees/s=69.94\n",
            "Round 394: train_corr=+0.1154 valid_corr=+0.0238 trees/s=42.50\n",
            "Round 395: train_corr=+0.1155 valid_corr=+0.0238 trees/s=44.48\n",
            "Round 396: train_corr=+0.1158 valid_corr=+0.0238 trees/s=58.65\n",
            "Round 397: train_corr=+0.1159 valid_corr=+0.0238 trees/s=49.09\n",
            "Round 398: train_corr=+0.1160 valid_corr=+0.0238 trees/s=47.35\n",
            "Round 399: train_corr=+0.1160 valid_corr=+0.0238 trees/s=63.48\n",
            "Round 400: train_corr=+0.1161 valid_corr=+0.0253 trees/s=1.71\n",
            "Round 401: train_corr=+0.1162 valid_corr=+0.0253 trees/s=58.04\n",
            "Round 402: train_corr=+0.1163 valid_corr=+0.0253 trees/s=56.93\n",
            "Round 403: train_corr=+0.1164 valid_corr=+0.0253 trees/s=44.04\n",
            "Round 404: train_corr=+0.1165 valid_corr=+0.0253 trees/s=53.44\n",
            "Round 405: train_corr=+0.1165 valid_corr=+0.0253 trees/s=60.40\n",
            "Round 406: train_corr=+0.1167 valid_corr=+0.0253 trees/s=49.01\n",
            "Round 407: train_corr=+0.1168 valid_corr=+0.0253 trees/s=42.18\n",
            "Round 408: train_corr=+0.1169 valid_corr=+0.0253 trees/s=47.89\n",
            "Round 409: train_corr=+0.1170 valid_corr=+0.0253 trees/s=39.97\n",
            "Round 410: train_corr=+0.1171 valid_corr=+0.0253 trees/s=44.10\n",
            "Round 411: train_corr=+0.1171 valid_corr=+0.0253 trees/s=52.73\n",
            "Round 412: train_corr=+0.1172 valid_corr=+0.0253 trees/s=60.65\n",
            "Round 413: train_corr=+0.1174 valid_corr=+0.0253 trees/s=42.82\n",
            "Round 414: train_corr=+0.1176 valid_corr=+0.0253 trees/s=38.07\n",
            "Round 415: train_corr=+0.1178 valid_corr=+0.0253 trees/s=50.88\n",
            "Round 416: train_corr=+0.1178 valid_corr=+0.0253 trees/s=59.22\n",
            "Round 417: train_corr=+0.1179 valid_corr=+0.0253 trees/s=65.40\n",
            "Round 418: train_corr=+0.1180 valid_corr=+0.0253 trees/s=51.56\n",
            "Round 419: train_corr=+0.1181 valid_corr=+0.0253 trees/s=50.45\n",
            "Round 420: train_corr=+0.1182 valid_corr=+0.0253 trees/s=56.02\n",
            "Round 421: train_corr=+0.1184 valid_corr=+0.0253 trees/s=51.30\n",
            "Round 422: train_corr=+0.1186 valid_corr=+0.0253 trees/s=41.22\n",
            "Round 423: train_corr=+0.1186 valid_corr=+0.0253 trees/s=59.80\n",
            "Round 424: train_corr=+0.1187 valid_corr=+0.0253 trees/s=66.06\n",
            "Round 425: train_corr=+0.1188 valid_corr=+0.0253 trees/s=58.60\n",
            "Round 426: train_corr=+0.1189 valid_corr=+0.0253 trees/s=46.00\n",
            "Round 427: train_corr=+0.1190 valid_corr=+0.0253 trees/s=42.16\n",
            "Round 428: train_corr=+0.1191 valid_corr=+0.0253 trees/s=66.25\n",
            "Round 429: train_corr=+0.1192 valid_corr=+0.0253 trees/s=63.18\n",
            "Round 430: train_corr=+0.1193 valid_corr=+0.0253 trees/s=47.56\n",
            "Round 431: train_corr=+0.1194 valid_corr=+0.0253 trees/s=56.65\n",
            "Round 432: train_corr=+0.1195 valid_corr=+0.0253 trees/s=55.36\n",
            "Round 433: train_corr=+0.1196 valid_corr=+0.0253 trees/s=47.90\n",
            "Round 434: train_corr=+0.1197 valid_corr=+0.0253 trees/s=59.50\n",
            "Round 435: train_corr=+0.1199 valid_corr=+0.0253 trees/s=44.46\n",
            "Round 436: train_corr=+0.1199 valid_corr=+0.0253 trees/s=52.47\n",
            "Round 437: train_corr=+0.1200 valid_corr=+0.0253 trees/s=62.34\n",
            "Round 438: train_corr=+0.1202 valid_corr=+0.0253 trees/s=55.17\n",
            "Round 439: train_corr=+0.1203 valid_corr=+0.0253 trees/s=49.96\n",
            "Round 440: train_corr=+0.1204 valid_corr=+0.0253 trees/s=52.83\n",
            "Round 441: train_corr=+0.1204 valid_corr=+0.0253 trees/s=60.00\n",
            "Round 442: train_corr=+0.1206 valid_corr=+0.0253 trees/s=48.28\n",
            "Round 443: train_corr=+0.1207 valid_corr=+0.0253 trees/s=60.96\n",
            "Round 444: train_corr=+0.1208 valid_corr=+0.0253 trees/s=56.62\n",
            "Round 445: train_corr=+0.1209 valid_corr=+0.0253 trees/s=67.65\n",
            "Round 446: train_corr=+0.1210 valid_corr=+0.0253 trees/s=64.26\n",
            "Round 447: train_corr=+0.1211 valid_corr=+0.0253 trees/s=51.55\n",
            "Round 448: train_corr=+0.1211 valid_corr=+0.0253 trees/s=58.20\n",
            "Round 449: train_corr=+0.1213 valid_corr=+0.0253 trees/s=57.33\n",
            "Round 450: train_corr=+0.1214 valid_corr=+0.0253 trees/s=49.54\n",
            "Round 451: train_corr=+0.1216 valid_corr=+0.0253 trees/s=49.04\n",
            "Round 452: train_corr=+0.1216 valid_corr=+0.0253 trees/s=49.08\n",
            "Round 453: train_corr=+0.1217 valid_corr=+0.0253 trees/s=73.04\n",
            "Round 454: train_corr=+0.1218 valid_corr=+0.0253 trees/s=52.91\n",
            "Round 455: train_corr=+0.1219 valid_corr=+0.0253 trees/s=53.55\n",
            "Round 456: train_corr=+0.1220 valid_corr=+0.0253 trees/s=45.85\n",
            "Round 457: train_corr=+0.1221 valid_corr=+0.0253 trees/s=68.24\n",
            "Round 458: train_corr=+0.1222 valid_corr=+0.0253 trees/s=55.73\n",
            "Round 459: train_corr=+0.1223 valid_corr=+0.0253 trees/s=51.48\n",
            "Round 460: train_corr=+0.1224 valid_corr=+0.0253 trees/s=54.85\n",
            "Round 461: train_corr=+0.1226 valid_corr=+0.0253 trees/s=40.22\n",
            "Round 462: train_corr=+0.1228 valid_corr=+0.0253 trees/s=56.33\n",
            "Round 463: train_corr=+0.1229 valid_corr=+0.0253 trees/s=46.07\n",
            "Round 464: train_corr=+0.1230 valid_corr=+0.0253 trees/s=43.43\n",
            "Round 465: train_corr=+0.1232 valid_corr=+0.0253 trees/s=40.55\n",
            "Round 466: train_corr=+0.1234 valid_corr=+0.0253 trees/s=41.37\n",
            "Round 467: train_corr=+0.1234 valid_corr=+0.0253 trees/s=59.76\n",
            "Round 468: train_corr=+0.1235 valid_corr=+0.0253 trees/s=51.88\n",
            "Round 469: train_corr=+0.1236 valid_corr=+0.0253 trees/s=49.87\n",
            "Round 470: train_corr=+0.1237 valid_corr=+0.0253 trees/s=57.06\n",
            "Round 471: train_corr=+0.1238 valid_corr=+0.0253 trees/s=72.82\n",
            "Round 472: train_corr=+0.1239 valid_corr=+0.0253 trees/s=53.27\n",
            "Round 473: train_corr=+0.1240 valid_corr=+0.0253 trees/s=59.94\n",
            "Round 474: train_corr=+0.1241 valid_corr=+0.0253 trees/s=49.08\n",
            "Round 475: train_corr=+0.1242 valid_corr=+0.0253 trees/s=47.05\n",
            "Round 476: train_corr=+0.1242 valid_corr=+0.0253 trees/s=60.52\n",
            "Round 477: train_corr=+0.1244 valid_corr=+0.0253 trees/s=41.01\n",
            "Round 478: train_corr=+0.1245 valid_corr=+0.0253 trees/s=66.06\n",
            "Round 479: train_corr=+0.1245 valid_corr=+0.0253 trees/s=63.34\n",
            "Round 480: train_corr=+0.1246 valid_corr=+0.0253 trees/s=56.48\n",
            "Round 481: train_corr=+0.1248 valid_corr=+0.0253 trees/s=46.19\n",
            "Round 482: train_corr=+0.1250 valid_corr=+0.0253 trees/s=39.03\n",
            "Round 483: train_corr=+0.1251 valid_corr=+0.0253 trees/s=46.70\n",
            "Round 484: train_corr=+0.1252 valid_corr=+0.0253 trees/s=45.25\n",
            "Round 485: train_corr=+0.1253 valid_corr=+0.0253 trees/s=45.97\n",
            "Round 486: train_corr=+0.1254 valid_corr=+0.0253 trees/s=46.05\n",
            "Round 487: train_corr=+0.1255 valid_corr=+0.0253 trees/s=63.24\n",
            "Round 488: train_corr=+0.1256 valid_corr=+0.0253 trees/s=44.86\n",
            "Round 489: train_corr=+0.1257 valid_corr=+0.0253 trees/s=51.51\n",
            "Round 490: train_corr=+0.1256 valid_corr=+0.0253 trees/s=56.14\n",
            "Round 491: train_corr=+0.1257 valid_corr=+0.0253 trees/s=56.93\n",
            "Round 492: train_corr=+0.1258 valid_corr=+0.0253 trees/s=46.14\n",
            "Round 493: train_corr=+0.1259 valid_corr=+0.0253 trees/s=59.81\n",
            "Round 494: train_corr=+0.1260 valid_corr=+0.0253 trees/s=45.01\n",
            "Round 495: train_corr=+0.1261 valid_corr=+0.0253 trees/s=59.41\n",
            "Round 496: train_corr=+0.1262 valid_corr=+0.0253 trees/s=55.01\n",
            "Round 497: train_corr=+0.1262 valid_corr=+0.0253 trees/s=39.61\n",
            "Round 498: train_corr=+0.1264 valid_corr=+0.0253 trees/s=56.55\n",
            "Round 499: train_corr=+0.1265 valid_corr=+0.0253 trees/s=57.47\n",
            "Round 500: train_corr=+0.1266 valid_corr=+0.0266 trees/s=1.38\n",
            "Round 501: train_corr=+0.1267 valid_corr=+0.0266 trees/s=57.88\n",
            "Round 502: train_corr=+0.1268 valid_corr=+0.0266 trees/s=60.98\n",
            "Round 503: train_corr=+0.1269 valid_corr=+0.0266 trees/s=57.13\n",
            "Round 504: train_corr=+0.1270 valid_corr=+0.0266 trees/s=63.91\n",
            "Round 505: train_corr=+0.1271 valid_corr=+0.0266 trees/s=55.13\n",
            "Round 506: train_corr=+0.1272 valid_corr=+0.0266 trees/s=35.32\n",
            "Round 507: train_corr=+0.1273 valid_corr=+0.0266 trees/s=55.27\n",
            "Round 508: train_corr=+0.1273 valid_corr=+0.0266 trees/s=60.82\n",
            "Round 509: train_corr=+0.1275 valid_corr=+0.0266 trees/s=57.92\n",
            "Round 510: train_corr=+0.1276 valid_corr=+0.0266 trees/s=46.22\n",
            "Round 511: train_corr=+0.1277 valid_corr=+0.0266 trees/s=45.24\n",
            "Round 512: train_corr=+0.1278 valid_corr=+0.0266 trees/s=49.34\n",
            "Round 513: train_corr=+0.1279 valid_corr=+0.0266 trees/s=54.26\n",
            "Round 514: train_corr=+0.1280 valid_corr=+0.0266 trees/s=43.62\n",
            "Round 515: train_corr=+0.1282 valid_corr=+0.0266 trees/s=45.49\n",
            "Round 516: train_corr=+0.1283 valid_corr=+0.0266 trees/s=44.19\n",
            "Round 517: train_corr=+0.1284 valid_corr=+0.0266 trees/s=56.80\n",
            "Round 518: train_corr=+0.1285 valid_corr=+0.0266 trees/s=54.78\n",
            "Round 519: train_corr=+0.1285 valid_corr=+0.0266 trees/s=66.86\n",
            "Round 520: train_corr=+0.1286 valid_corr=+0.0266 trees/s=44.51\n",
            "Round 521: train_corr=+0.1286 valid_corr=+0.0266 trees/s=59.23\n",
            "Round 522: train_corr=+0.1288 valid_corr=+0.0266 trees/s=54.15\n",
            "Round 523: train_corr=+0.1288 valid_corr=+0.0266 trees/s=49.55\n",
            "Round 524: train_corr=+0.1289 valid_corr=+0.0266 trees/s=59.67\n",
            "Round 525: train_corr=+0.1290 valid_corr=+0.0266 trees/s=58.92\n",
            "Round 526: train_corr=+0.1291 valid_corr=+0.0266 trees/s=52.70\n",
            "Round 527: train_corr=+0.1292 valid_corr=+0.0266 trees/s=56.75\n",
            "Round 528: train_corr=+0.1293 valid_corr=+0.0266 trees/s=43.32\n",
            "Round 529: train_corr=+0.1294 valid_corr=+0.0266 trees/s=57.65\n",
            "Round 530: train_corr=+0.1296 valid_corr=+0.0266 trees/s=39.29\n",
            "Round 531: train_corr=+0.1297 valid_corr=+0.0266 trees/s=56.32\n",
            "Round 532: train_corr=+0.1298 valid_corr=+0.0266 trees/s=55.80\n",
            "Round 533: train_corr=+0.1299 valid_corr=+0.0266 trees/s=51.99\n",
            "Round 534: train_corr=+0.1300 valid_corr=+0.0266 trees/s=60.65\n",
            "Round 535: train_corr=+0.1302 valid_corr=+0.0266 trees/s=42.89\n",
            "Round 536: train_corr=+0.1303 valid_corr=+0.0266 trees/s=56.32\n",
            "Round 537: train_corr=+0.1304 valid_corr=+0.0266 trees/s=44.44\n",
            "Round 538: train_corr=+0.1305 valid_corr=+0.0266 trees/s=47.95\n",
            "Round 539: train_corr=+0.1306 valid_corr=+0.0266 trees/s=62.04\n",
            "Round 540: train_corr=+0.1308 valid_corr=+0.0266 trees/s=41.10\n",
            "Round 541: train_corr=+0.1308 valid_corr=+0.0266 trees/s=55.95\n",
            "Round 542: train_corr=+0.1309 valid_corr=+0.0266 trees/s=51.05\n",
            "Round 543: train_corr=+0.1310 valid_corr=+0.0266 trees/s=69.12\n",
            "Round 544: train_corr=+0.1312 valid_corr=+0.0266 trees/s=48.41\n",
            "Round 545: train_corr=+0.1312 valid_corr=+0.0266 trees/s=65.55\n",
            "Round 546: train_corr=+0.1313 valid_corr=+0.0266 trees/s=46.22\n",
            "Round 547: train_corr=+0.1313 valid_corr=+0.0266 trees/s=57.85\n",
            "Round 548: train_corr=+0.1314 valid_corr=+0.0266 trees/s=50.81\n",
            "Round 549: train_corr=+0.1315 valid_corr=+0.0266 trees/s=43.05\n",
            "Round 550: train_corr=+0.1317 valid_corr=+0.0266 trees/s=48.49\n",
            "Round 551: train_corr=+0.1318 valid_corr=+0.0266 trees/s=42.28\n",
            "Round 552: train_corr=+0.1319 valid_corr=+0.0266 trees/s=60.96\n",
            "Round 553: train_corr=+0.1320 valid_corr=+0.0266 trees/s=45.02\n",
            "Round 554: train_corr=+0.1321 valid_corr=+0.0266 trees/s=42.51\n",
            "Round 555: train_corr=+0.1321 valid_corr=+0.0266 trees/s=45.87\n",
            "Round 556: train_corr=+0.1322 valid_corr=+0.0266 trees/s=45.28\n",
            "Round 557: train_corr=+0.1323 valid_corr=+0.0266 trees/s=63.27\n",
            "Round 558: train_corr=+0.1323 valid_corr=+0.0266 trees/s=56.41\n",
            "Round 559: train_corr=+0.1324 valid_corr=+0.0266 trees/s=60.59\n",
            "Round 560: train_corr=+0.1326 valid_corr=+0.0266 trees/s=43.46\n",
            "Round 561: train_corr=+0.1326 valid_corr=+0.0266 trees/s=53.15\n",
            "Round 562: train_corr=+0.1326 valid_corr=+0.0266 trees/s=66.51\n",
            "Round 563: train_corr=+0.1327 valid_corr=+0.0266 trees/s=51.08\n",
            "Round 564: train_corr=+0.1328 valid_corr=+0.0266 trees/s=68.04\n",
            "Round 565: train_corr=+0.1328 valid_corr=+0.0266 trees/s=64.04\n",
            "Round 566: train_corr=+0.1329 valid_corr=+0.0266 trees/s=48.10\n",
            "Round 567: train_corr=+0.1331 valid_corr=+0.0266 trees/s=45.86\n",
            "Round 568: train_corr=+0.1332 valid_corr=+0.0266 trees/s=42.80\n",
            "Round 569: train_corr=+0.1333 valid_corr=+0.0266 trees/s=64.09\n",
            "Round 570: train_corr=+0.1334 valid_corr=+0.0266 trees/s=39.74\n",
            "Round 571: train_corr=+0.1335 valid_corr=+0.0266 trees/s=62.28\n",
            "Round 572: train_corr=+0.1336 valid_corr=+0.0266 trees/s=46.45\n",
            "Round 573: train_corr=+0.1337 valid_corr=+0.0266 trees/s=46.27\n",
            "Round 574: train_corr=+0.1338 valid_corr=+0.0266 trees/s=46.47\n",
            "Round 575: train_corr=+0.1339 valid_corr=+0.0266 trees/s=45.35\n",
            "Round 576: train_corr=+0.1339 valid_corr=+0.0266 trees/s=63.78\n",
            "Round 577: train_corr=+0.1340 valid_corr=+0.0266 trees/s=56.82\n",
            "Round 578: train_corr=+0.1341 valid_corr=+0.0266 trees/s=54.53\n",
            "Round 579: train_corr=+0.1341 valid_corr=+0.0266 trees/s=67.35\n",
            "Round 580: train_corr=+0.1342 valid_corr=+0.0266 trees/s=51.42\n",
            "Round 581: train_corr=+0.1342 valid_corr=+0.0266 trees/s=59.63\n",
            "Round 582: train_corr=+0.1343 valid_corr=+0.0266 trees/s=45.40\n",
            "Round 583: train_corr=+0.1343 valid_corr=+0.0266 trees/s=58.02\n",
            "Round 584: train_corr=+0.1344 valid_corr=+0.0266 trees/s=56.42\n",
            "Round 585: train_corr=+0.1345 valid_corr=+0.0266 trees/s=48.94\n",
            "Round 586: train_corr=+0.1345 valid_corr=+0.0266 trees/s=64.35\n",
            "Round 587: train_corr=+0.1346 valid_corr=+0.0266 trees/s=58.18\n",
            "Round 588: train_corr=+0.1347 valid_corr=+0.0266 trees/s=65.08\n",
            "Round 589: train_corr=+0.1347 valid_corr=+0.0266 trees/s=53.78\n",
            "Round 590: train_corr=+0.1348 valid_corr=+0.0266 trees/s=63.43\n",
            "Round 591: train_corr=+0.1348 valid_corr=+0.0266 trees/s=52.17\n",
            "Round 592: train_corr=+0.1349 valid_corr=+0.0266 trees/s=56.51\n",
            "Round 593: train_corr=+0.1350 valid_corr=+0.0266 trees/s=73.42\n",
            "Round 594: train_corr=+0.1351 valid_corr=+0.0266 trees/s=49.74\n",
            "Round 595: train_corr=+0.1352 valid_corr=+0.0266 trees/s=54.51\n",
            "Round 596: train_corr=+0.1354 valid_corr=+0.0266 trees/s=47.74\n",
            "Round 597: train_corr=+0.1355 valid_corr=+0.0266 trees/s=51.70\n",
            "Round 598: train_corr=+0.1356 valid_corr=+0.0266 trees/s=57.04\n",
            "Round 599: train_corr=+0.1358 valid_corr=+0.0266 trees/s=44.99\n",
            "Round 600: train_corr=+0.1358 valid_corr=+0.0272 trees/s=1.15\n",
            "Round 601: train_corr=+0.1359 valid_corr=+0.0272 trees/s=43.67\n",
            "Round 602: train_corr=+0.1360 valid_corr=+0.0272 trees/s=54.04\n",
            "Round 603: train_corr=+0.1362 valid_corr=+0.0272 trees/s=50.83\n",
            "Round 604: train_corr=+0.1363 valid_corr=+0.0272 trees/s=50.42\n",
            "Round 605: train_corr=+0.1363 valid_corr=+0.0272 trees/s=54.33\n",
            "Round 606: train_corr=+0.1364 valid_corr=+0.0272 trees/s=46.15\n",
            "Round 607: train_corr=+0.1365 valid_corr=+0.0272 trees/s=52.40\n",
            "Round 608: train_corr=+0.1366 valid_corr=+0.0272 trees/s=57.90\n",
            "Round 609: train_corr=+0.1367 valid_corr=+0.0272 trees/s=45.53\n",
            "Round 610: train_corr=+0.1367 valid_corr=+0.0272 trees/s=57.88\n",
            "Round 611: train_corr=+0.1369 valid_corr=+0.0272 trees/s=56.24\n",
            "Round 612: train_corr=+0.1369 valid_corr=+0.0272 trees/s=67.20\n",
            "Round 613: train_corr=+0.1370 valid_corr=+0.0272 trees/s=53.71\n",
            "Round 614: train_corr=+0.1370 valid_corr=+0.0272 trees/s=63.83\n",
            "Round 615: train_corr=+0.1371 valid_corr=+0.0272 trees/s=47.86\n",
            "Round 616: train_corr=+0.1372 valid_corr=+0.0272 trees/s=51.78\n",
            "Round 617: train_corr=+0.1373 valid_corr=+0.0272 trees/s=58.69\n",
            "Round 618: train_corr=+0.1374 valid_corr=+0.0272 trees/s=57.73\n",
            "Round 619: train_corr=+0.1375 valid_corr=+0.0272 trees/s=61.27\n",
            "Round 620: train_corr=+0.1375 valid_corr=+0.0272 trees/s=61.05\n",
            "Round 621: train_corr=+0.1376 valid_corr=+0.0272 trees/s=52.82\n",
            "Round 622: train_corr=+0.1376 valid_corr=+0.0272 trees/s=54.59\n",
            "Round 623: train_corr=+0.1377 valid_corr=+0.0272 trees/s=41.34\n",
            "Round 624: train_corr=+0.1377 valid_corr=+0.0272 trees/s=55.81\n",
            "Round 625: train_corr=+0.1378 valid_corr=+0.0272 trees/s=61.68\n",
            "Round 626: train_corr=+0.1379 valid_corr=+0.0272 trees/s=62.06\n",
            "Round 627: train_corr=+0.1380 valid_corr=+0.0272 trees/s=53.58\n",
            "Round 628: train_corr=+0.1381 valid_corr=+0.0272 trees/s=50.26\n",
            "Round 629: train_corr=+0.1381 valid_corr=+0.0272 trees/s=69.71\n",
            "Round 630: train_corr=+0.1383 valid_corr=+0.0272 trees/s=47.43\n",
            "Round 631: train_corr=+0.1384 valid_corr=+0.0272 trees/s=48.82\n",
            "Round 632: train_corr=+0.1385 valid_corr=+0.0272 trees/s=40.35\n",
            "Round 633: train_corr=+0.1386 valid_corr=+0.0272 trees/s=44.47\n",
            "Round 634: train_corr=+0.1387 valid_corr=+0.0272 trees/s=49.87\n",
            "Round 635: train_corr=+0.1388 valid_corr=+0.0272 trees/s=53.35\n",
            "Round 636: train_corr=+0.1389 valid_corr=+0.0272 trees/s=42.08\n",
            "Round 637: train_corr=+0.1390 valid_corr=+0.0272 trees/s=51.13\n",
            "Round 638: train_corr=+0.1391 valid_corr=+0.0272 trees/s=59.36\n",
            "Round 639: train_corr=+0.1392 valid_corr=+0.0272 trees/s=55.03\n",
            "Round 640: train_corr=+0.1392 valid_corr=+0.0272 trees/s=58.68\n",
            "Round 641: train_corr=+0.1392 valid_corr=+0.0272 trees/s=51.87\n",
            "Round 642: train_corr=+0.1393 valid_corr=+0.0272 trees/s=49.82\n",
            "Round 643: train_corr=+0.1394 valid_corr=+0.0272 trees/s=41.44\n",
            "Round 644: train_corr=+0.1395 valid_corr=+0.0272 trees/s=67.35\n",
            "Round 645: train_corr=+0.1396 valid_corr=+0.0272 trees/s=44.88\n",
            "Round 646: train_corr=+0.1397 valid_corr=+0.0272 trees/s=47.32\n",
            "Round 647: train_corr=+0.1398 valid_corr=+0.0272 trees/s=48.83\n",
            "Round 648: train_corr=+0.1401 valid_corr=+0.0272 trees/s=41.55\n",
            "Round 649: train_corr=+0.1402 valid_corr=+0.0272 trees/s=48.83\n",
            "Round 650: train_corr=+0.1402 valid_corr=+0.0272 trees/s=63.78\n",
            "Round 651: train_corr=+0.1403 valid_corr=+0.0272 trees/s=50.84\n",
            "Round 652: train_corr=+0.1403 valid_corr=+0.0272 trees/s=65.70\n",
            "Round 653: train_corr=+0.1403 valid_corr=+0.0272 trees/s=64.03\n",
            "Round 654: train_corr=+0.1404 valid_corr=+0.0272 trees/s=37.96\n",
            "Round 655: train_corr=+0.1404 valid_corr=+0.0272 trees/s=50.02\n",
            "Round 656: train_corr=+0.1405 valid_corr=+0.0272 trees/s=44.71\n",
            "Round 657: train_corr=+0.1407 valid_corr=+0.0272 trees/s=41.99\n",
            "Round 658: train_corr=+0.1408 valid_corr=+0.0272 trees/s=52.43\n",
            "Round 659: train_corr=+0.1409 valid_corr=+0.0272 trees/s=58.03\n",
            "Round 660: train_corr=+0.1411 valid_corr=+0.0272 trees/s=40.78\n",
            "Round 661: train_corr=+0.1411 valid_corr=+0.0272 trees/s=59.93\n",
            "Round 662: train_corr=+0.1412 valid_corr=+0.0272 trees/s=55.03\n",
            "Round 663: train_corr=+0.1412 valid_corr=+0.0272 trees/s=63.93\n",
            "Round 664: train_corr=+0.1414 valid_corr=+0.0272 trees/s=53.52\n",
            "Round 665: train_corr=+0.1415 valid_corr=+0.0272 trees/s=57.25\n",
            "Round 666: train_corr=+0.1415 valid_corr=+0.0272 trees/s=64.79\n",
            "Round 667: train_corr=+0.1415 valid_corr=+0.0272 trees/s=55.12\n",
            "Round 668: train_corr=+0.1416 valid_corr=+0.0272 trees/s=46.62\n",
            "Round 669: train_corr=+0.1416 valid_corr=+0.0272 trees/s=67.95\n",
            "Round 670: train_corr=+0.1418 valid_corr=+0.0272 trees/s=42.93\n",
            "Round 671: train_corr=+0.1418 valid_corr=+0.0272 trees/s=54.33\n",
            "Round 672: train_corr=+0.1419 valid_corr=+0.0272 trees/s=71.70\n",
            "Round 673: train_corr=+0.1421 valid_corr=+0.0272 trees/s=41.51\n",
            "Round 674: train_corr=+0.1422 valid_corr=+0.0272 trees/s=50.19\n",
            "Round 675: train_corr=+0.1424 valid_corr=+0.0272 trees/s=45.77\n",
            "Round 676: train_corr=+0.1425 valid_corr=+0.0272 trees/s=50.09\n",
            "Round 677: train_corr=+0.1426 valid_corr=+0.0272 trees/s=50.97\n",
            "Round 678: train_corr=+0.1427 valid_corr=+0.0272 trees/s=43.59\n",
            "Round 679: train_corr=+0.1427 valid_corr=+0.0272 trees/s=55.39\n",
            "Round 680: train_corr=+0.1428 valid_corr=+0.0272 trees/s=53.32\n",
            "Round 681: train_corr=+0.1428 valid_corr=+0.0272 trees/s=63.55\n",
            "Round 682: train_corr=+0.1430 valid_corr=+0.0272 trees/s=49.45\n",
            "Round 683: train_corr=+0.1431 valid_corr=+0.0272 trees/s=38.87\n",
            "Round 684: train_corr=+0.1432 valid_corr=+0.0272 trees/s=50.00\n",
            "Round 685: train_corr=+0.1433 valid_corr=+0.0272 trees/s=53.41\n",
            "Round 686: train_corr=+0.1433 valid_corr=+0.0272 trees/s=60.47\n",
            "Round 687: train_corr=+0.1434 valid_corr=+0.0272 trees/s=50.06\n",
            "Round 688: train_corr=+0.1434 valid_corr=+0.0272 trees/s=61.46\n",
            "Round 689: train_corr=+0.1435 valid_corr=+0.0272 trees/s=47.62\n",
            "Round 690: train_corr=+0.1436 valid_corr=+0.0272 trees/s=51.55\n",
            "Round 691: train_corr=+0.1437 valid_corr=+0.0272 trees/s=44.18\n",
            "Round 692: train_corr=+0.1438 valid_corr=+0.0272 trees/s=41.18\n",
            "Round 693: train_corr=+0.1438 valid_corr=+0.0272 trees/s=60.14\n",
            "Round 694: train_corr=+0.1439 valid_corr=+0.0272 trees/s=62.01\n",
            "Round 695: train_corr=+0.1440 valid_corr=+0.0272 trees/s=55.82\n",
            "Round 696: train_corr=+0.1440 valid_corr=+0.0272 trees/s=55.89\n",
            "Round 697: train_corr=+0.1442 valid_corr=+0.0272 trees/s=42.01\n",
            "Round 698: train_corr=+0.1443 valid_corr=+0.0272 trees/s=51.24\n",
            "Round 699: train_corr=+0.1444 valid_corr=+0.0272 trees/s=42.63\n",
            "Round 700: train_corr=+0.1445 valid_corr=+0.0276 trees/s=0.98\n",
            "Round 701: train_corr=+0.1446 valid_corr=+0.0276 trees/s=63.02\n",
            "Round 702: train_corr=+0.1447 valid_corr=+0.0276 trees/s=51.57\n",
            "Round 703: train_corr=+0.1447 valid_corr=+0.0276 trees/s=48.06\n",
            "Round 704: train_corr=+0.1448 valid_corr=+0.0276 trees/s=65.10\n",
            "Round 705: train_corr=+0.1449 valid_corr=+0.0276 trees/s=57.62\n",
            "Round 706: train_corr=+0.1450 valid_corr=+0.0276 trees/s=56.34\n",
            "Round 707: train_corr=+0.1450 valid_corr=+0.0276 trees/s=48.30\n",
            "Round 708: train_corr=+0.1451 valid_corr=+0.0276 trees/s=58.73\n",
            "Round 709: train_corr=+0.1452 valid_corr=+0.0276 trees/s=51.37\n",
            "Round 710: train_corr=+0.1452 valid_corr=+0.0276 trees/s=51.89\n",
            "Round 711: train_corr=+0.1453 valid_corr=+0.0276 trees/s=41.90\n",
            "Round 712: train_corr=+0.1454 valid_corr=+0.0276 trees/s=50.54\n",
            "Round 713: train_corr=+0.1454 valid_corr=+0.0276 trees/s=52.98\n",
            "Round 714: train_corr=+0.1455 valid_corr=+0.0276 trees/s=72.53\n",
            "Round 715: train_corr=+0.1456 valid_corr=+0.0276 trees/s=51.94\n",
            "Round 716: train_corr=+0.1457 valid_corr=+0.0276 trees/s=50.29\n",
            "Round 717: train_corr=+0.1457 valid_corr=+0.0276 trees/s=48.69\n",
            "Round 718: train_corr=+0.1458 valid_corr=+0.0276 trees/s=58.70\n",
            "Round 719: train_corr=+0.1459 valid_corr=+0.0276 trees/s=72.51\n",
            "Round 720: train_corr=+0.1461 valid_corr=+0.0276 trees/s=48.29\n",
            "Round 721: train_corr=+0.1461 valid_corr=+0.0276 trees/s=52.75\n",
            "Round 722: train_corr=+0.1462 valid_corr=+0.0276 trees/s=63.39\n",
            "Round 723: train_corr=+0.1462 valid_corr=+0.0276 trees/s=53.23\n",
            "Round 724: train_corr=+0.1463 valid_corr=+0.0276 trees/s=53.07\n",
            "Round 725: train_corr=+0.1464 valid_corr=+0.0276 trees/s=43.66\n",
            "Round 726: train_corr=+0.1464 valid_corr=+0.0276 trees/s=64.85\n",
            "Round 727: train_corr=+0.1465 valid_corr=+0.0276 trees/s=71.66\n",
            "Round 728: train_corr=+0.1466 valid_corr=+0.0276 trees/s=45.00\n",
            "Round 729: train_corr=+0.1466 valid_corr=+0.0276 trees/s=54.06\n",
            "Round 730: train_corr=+0.1468 valid_corr=+0.0276 trees/s=44.95\n",
            "Round 731: train_corr=+0.1468 valid_corr=+0.0276 trees/s=58.53\n",
            "Round 732: train_corr=+0.1469 valid_corr=+0.0276 trees/s=43.15\n",
            "Round 733: train_corr=+0.1470 valid_corr=+0.0276 trees/s=45.82\n",
            "Round 734: train_corr=+0.1471 valid_corr=+0.0276 trees/s=44.34\n",
            "Round 735: train_corr=+0.1472 valid_corr=+0.0276 trees/s=43.22\n",
            "Round 736: train_corr=+0.1473 valid_corr=+0.0276 trees/s=48.79\n",
            "Round 737: train_corr=+0.1473 valid_corr=+0.0276 trees/s=54.08\n",
            "Round 738: train_corr=+0.1474 valid_corr=+0.0276 trees/s=61.41\n",
            "Round 739: train_corr=+0.1474 valid_corr=+0.0276 trees/s=62.44\n",
            "Round 740: train_corr=+0.1475 valid_corr=+0.0276 trees/s=58.02\n",
            "Round 741: train_corr=+0.1476 valid_corr=+0.0276 trees/s=43.75\n",
            "Round 742: train_corr=+0.1476 valid_corr=+0.0276 trees/s=66.48\n",
            "Round 743: train_corr=+0.1477 valid_corr=+0.0276 trees/s=49.97\n",
            "Round 744: train_corr=+0.1478 valid_corr=+0.0276 trees/s=52.12\n",
            "Round 745: train_corr=+0.1479 valid_corr=+0.0276 trees/s=44.02\n",
            "Round 746: train_corr=+0.1479 valid_corr=+0.0276 trees/s=57.32\n",
            "Round 747: train_corr=+0.1480 valid_corr=+0.0276 trees/s=61.70\n",
            "Round 748: train_corr=+0.1480 valid_corr=+0.0276 trees/s=59.32\n",
            "Round 749: train_corr=+0.1481 valid_corr=+0.0276 trees/s=52.42\n",
            "Round 750: train_corr=+0.1481 valid_corr=+0.0276 trees/s=62.39\n",
            "Round 751: train_corr=+0.1482 valid_corr=+0.0276 trees/s=48.98\n",
            "Round 752: train_corr=+0.1483 valid_corr=+0.0276 trees/s=57.76\n",
            "Round 753: train_corr=+0.1484 valid_corr=+0.0276 trees/s=57.65\n",
            "Round 754: train_corr=+0.1485 valid_corr=+0.0276 trees/s=47.73\n",
            "Round 755: train_corr=+0.1485 valid_corr=+0.0276 trees/s=45.72\n",
            "Round 756: train_corr=+0.1486 valid_corr=+0.0276 trees/s=53.48\n",
            "Round 757: train_corr=+0.1487 valid_corr=+0.0276 trees/s=57.02\n",
            "Round 758: train_corr=+0.1488 valid_corr=+0.0276 trees/s=47.89\n",
            "Round 759: train_corr=+0.1489 valid_corr=+0.0276 trees/s=38.09\n",
            "Round 760: train_corr=+0.1490 valid_corr=+0.0276 trees/s=65.39\n",
            "Round 761: train_corr=+0.1490 valid_corr=+0.0276 trees/s=59.30\n",
            "Round 762: train_corr=+0.1491 valid_corr=+0.0276 trees/s=62.72\n",
            "Round 763: train_corr=+0.1492 valid_corr=+0.0276 trees/s=56.48\n",
            "Round 764: train_corr=+0.1493 valid_corr=+0.0276 trees/s=49.87\n",
            "Round 765: train_corr=+0.1493 valid_corr=+0.0276 trees/s=41.38\n",
            "Round 766: train_corr=+0.1494 valid_corr=+0.0276 trees/s=47.40\n",
            "Round 767: train_corr=+0.1495 valid_corr=+0.0276 trees/s=48.78\n",
            "Round 768: train_corr=+0.1496 valid_corr=+0.0276 trees/s=40.84\n",
            "Round 769: train_corr=+0.1496 valid_corr=+0.0276 trees/s=59.06\n",
            "Round 770: train_corr=+0.1497 valid_corr=+0.0276 trees/s=63.07\n",
            "Round 771: train_corr=+0.1497 valid_corr=+0.0276 trees/s=63.57\n",
            "Round 772: train_corr=+0.1497 valid_corr=+0.0276 trees/s=60.14\n",
            "Round 773: train_corr=+0.1499 valid_corr=+0.0276 trees/s=58.94\n",
            "Round 774: train_corr=+0.1500 valid_corr=+0.0276 trees/s=54.44\n",
            "Round 775: train_corr=+0.1501 valid_corr=+0.0276 trees/s=45.30\n",
            "Round 776: train_corr=+0.1501 valid_corr=+0.0276 trees/s=49.98\n",
            "Round 777: train_corr=+0.1502 valid_corr=+0.0276 trees/s=48.42\n",
            "Round 778: train_corr=+0.1503 valid_corr=+0.0276 trees/s=52.31\n",
            "Round 779: train_corr=+0.1504 valid_corr=+0.0276 trees/s=44.54\n",
            "Round 780: train_corr=+0.1505 valid_corr=+0.0276 trees/s=48.83\n",
            "Round 781: train_corr=+0.1506 valid_corr=+0.0276 trees/s=44.17\n",
            "Round 782: train_corr=+0.1507 valid_corr=+0.0276 trees/s=45.25\n",
            "Round 783: train_corr=+0.1507 valid_corr=+0.0276 trees/s=56.43\n",
            "Round 784: train_corr=+0.1508 valid_corr=+0.0276 trees/s=70.04\n",
            "Round 785: train_corr=+0.1509 valid_corr=+0.0276 trees/s=64.82\n",
            "Round 786: train_corr=+0.1509 valid_corr=+0.0276 trees/s=53.08\n",
            "Round 787: train_corr=+0.1510 valid_corr=+0.0276 trees/s=55.95\n",
            "Round 788: train_corr=+0.1511 valid_corr=+0.0276 trees/s=57.57\n",
            "Round 789: train_corr=+0.1512 valid_corr=+0.0276 trees/s=63.54\n",
            "Round 790: train_corr=+0.1513 valid_corr=+0.0276 trees/s=54.83\n",
            "Round 791: train_corr=+0.1513 valid_corr=+0.0276 trees/s=44.12\n",
            "Round 792: train_corr=+0.1514 valid_corr=+0.0276 trees/s=51.56\n",
            "Round 793: train_corr=+0.1515 valid_corr=+0.0276 trees/s=60.43\n",
            "Round 794: train_corr=+0.1516 valid_corr=+0.0276 trees/s=46.56\n",
            "Round 795: train_corr=+0.1516 valid_corr=+0.0276 trees/s=45.17\n",
            "Round 796: train_corr=+0.1518 valid_corr=+0.0276 trees/s=35.30\n",
            "Round 797: train_corr=+0.1519 valid_corr=+0.0276 trees/s=41.98\n",
            "Round 798: train_corr=+0.1520 valid_corr=+0.0276 trees/s=55.12\n",
            "Round 799: train_corr=+0.1520 valid_corr=+0.0276 trees/s=59.14\n",
            "Round 800: train_corr=+0.1521 valid_corr=+0.0279 trees/s=0.86\n",
            "Round 801: train_corr=+0.1522 valid_corr=+0.0279 trees/s=53.10\n",
            "Round 802: train_corr=+0.1523 valid_corr=+0.0279 trees/s=46.86\n",
            "Round 803: train_corr=+0.1524 valid_corr=+0.0279 trees/s=58.25\n",
            "Round 804: train_corr=+0.1524 valid_corr=+0.0279 trees/s=48.42\n",
            "Round 805: train_corr=+0.1525 valid_corr=+0.0279 trees/s=46.10\n",
            "Round 806: train_corr=+0.1526 valid_corr=+0.0279 trees/s=47.12\n",
            "Round 807: train_corr=+0.1527 valid_corr=+0.0279 trees/s=43.93\n",
            "Round 808: train_corr=+0.1528 valid_corr=+0.0279 trees/s=41.82\n",
            "Round 809: train_corr=+0.1528 valid_corr=+0.0279 trees/s=47.11\n",
            "Round 810: train_corr=+0.1529 valid_corr=+0.0279 trees/s=53.09\n",
            "Round 811: train_corr=+0.1529 valid_corr=+0.0279 trees/s=70.04\n",
            "Round 812: train_corr=+0.1530 valid_corr=+0.0279 trees/s=41.00\n",
            "Round 813: train_corr=+0.1532 valid_corr=+0.0279 trees/s=38.75\n",
            "Round 814: train_corr=+0.1533 valid_corr=+0.0279 trees/s=53.43\n",
            "Round 815: train_corr=+0.1533 valid_corr=+0.0279 trees/s=41.40\n",
            "Round 816: train_corr=+0.1534 valid_corr=+0.0279 trees/s=65.67\n",
            "Round 817: train_corr=+0.1534 valid_corr=+0.0279 trees/s=58.75\n",
            "Round 818: train_corr=+0.1535 valid_corr=+0.0279 trees/s=57.61\n",
            "Round 819: train_corr=+0.1535 valid_corr=+0.0279 trees/s=49.47\n",
            "Round 820: train_corr=+0.1536 valid_corr=+0.0279 trees/s=68.21\n",
            "Round 821: train_corr=+0.1537 valid_corr=+0.0279 trees/s=40.41\n",
            "Round 822: train_corr=+0.1539 valid_corr=+0.0279 trees/s=39.17\n",
            "Round 823: train_corr=+0.1540 valid_corr=+0.0279 trees/s=62.99\n",
            "Round 824: train_corr=+0.1540 valid_corr=+0.0279 trees/s=64.03\n",
            "Round 825: train_corr=+0.1541 valid_corr=+0.0279 trees/s=45.21\n",
            "Round 826: train_corr=+0.1543 valid_corr=+0.0279 trees/s=55.94\n",
            "Round 827: train_corr=+0.1543 valid_corr=+0.0279 trees/s=45.65\n",
            "Round 828: train_corr=+0.1544 valid_corr=+0.0279 trees/s=45.14\n",
            "Round 829: train_corr=+0.1545 valid_corr=+0.0279 trees/s=42.57\n",
            "Round 830: train_corr=+0.1546 valid_corr=+0.0279 trees/s=45.35\n",
            "Round 831: train_corr=+0.1547 valid_corr=+0.0279 trees/s=51.62\n",
            "Round 832: train_corr=+0.1547 valid_corr=+0.0279 trees/s=57.55\n",
            "Round 833: train_corr=+0.1548 valid_corr=+0.0279 trees/s=55.23\n",
            "Round 834: train_corr=+0.1549 valid_corr=+0.0279 trees/s=43.07\n",
            "Round 835: train_corr=+0.1549 valid_corr=+0.0279 trees/s=53.69\n",
            "Round 836: train_corr=+0.1551 valid_corr=+0.0279 trees/s=45.11\n",
            "Round 837: train_corr=+0.1551 valid_corr=+0.0279 trees/s=62.44\n",
            "Round 838: train_corr=+0.1552 valid_corr=+0.0279 trees/s=54.82\n",
            "Round 839: train_corr=+0.1553 valid_corr=+0.0279 trees/s=50.88\n",
            "Round 840: train_corr=+0.1553 valid_corr=+0.0279 trees/s=42.11\n",
            "Round 841: train_corr=+0.1554 valid_corr=+0.0279 trees/s=49.51\n",
            "Round 842: train_corr=+0.1555 valid_corr=+0.0279 trees/s=57.75\n",
            "Round 843: train_corr=+0.1555 valid_corr=+0.0279 trees/s=68.18\n",
            "Round 844: train_corr=+0.1556 valid_corr=+0.0279 trees/s=57.18\n",
            "Round 845: train_corr=+0.1556 valid_corr=+0.0279 trees/s=44.06\n",
            "Round 846: train_corr=+0.1557 valid_corr=+0.0279 trees/s=56.31\n",
            "Round 847: train_corr=+0.1558 valid_corr=+0.0279 trees/s=54.26\n",
            "Round 848: train_corr=+0.1559 valid_corr=+0.0279 trees/s=55.93\n",
            "Round 849: train_corr=+0.1560 valid_corr=+0.0279 trees/s=52.27\n",
            "Round 850: train_corr=+0.1561 valid_corr=+0.0279 trees/s=47.18\n",
            "Round 851: train_corr=+0.1561 valid_corr=+0.0279 trees/s=56.28\n",
            "Round 852: train_corr=+0.1562 valid_corr=+0.0279 trees/s=48.75\n",
            "Round 853: train_corr=+0.1563 valid_corr=+0.0279 trees/s=63.06\n",
            "Round 854: train_corr=+0.1564 valid_corr=+0.0279 trees/s=49.05\n",
            "Round 855: train_corr=+0.1565 valid_corr=+0.0279 trees/s=42.31\n",
            "Round 856: train_corr=+0.1565 valid_corr=+0.0279 trees/s=69.87\n",
            "Round 857: train_corr=+0.1566 valid_corr=+0.0279 trees/s=47.18\n",
            "Round 858: train_corr=+0.1566 valid_corr=+0.0279 trees/s=60.67\n",
            "Round 859: train_corr=+0.1567 valid_corr=+0.0279 trees/s=56.55\n",
            "Round 860: train_corr=+0.1569 valid_corr=+0.0279 trees/s=48.72\n",
            "Round 861: train_corr=+0.1569 valid_corr=+0.0279 trees/s=49.20\n",
            "Round 862: train_corr=+0.1570 valid_corr=+0.0279 trees/s=56.72\n",
            "Round 863: train_corr=+0.1571 valid_corr=+0.0279 trees/s=59.02\n",
            "Round 864: train_corr=+0.1571 valid_corr=+0.0279 trees/s=44.08\n",
            "Round 865: train_corr=+0.1572 valid_corr=+0.0279 trees/s=44.47\n",
            "Round 866: train_corr=+0.1572 valid_corr=+0.0279 trees/s=20.05\n",
            "Round 867: train_corr=+0.1573 valid_corr=+0.0279 trees/s=54.70\n",
            "Round 868: train_corr=+0.1574 valid_corr=+0.0279 trees/s=67.21\n",
            "Round 869: train_corr=+0.1575 valid_corr=+0.0279 trees/s=55.71\n",
            "Round 870: train_corr=+0.1575 valid_corr=+0.0279 trees/s=55.28\n",
            "Round 871: train_corr=+0.1575 valid_corr=+0.0279 trees/s=70.03\n",
            "Round 872: train_corr=+0.1577 valid_corr=+0.0279 trees/s=44.65\n",
            "Round 873: train_corr=+0.1577 valid_corr=+0.0279 trees/s=66.53\n",
            "Round 874: train_corr=+0.1578 valid_corr=+0.0279 trees/s=73.43\n",
            "Round 875: train_corr=+0.1578 valid_corr=+0.0279 trees/s=59.05\n",
            "Round 876: train_corr=+0.1579 valid_corr=+0.0279 trees/s=47.27\n",
            "Round 877: train_corr=+0.1580 valid_corr=+0.0279 trees/s=50.16\n",
            "Round 878: train_corr=+0.1581 valid_corr=+0.0279 trees/s=44.07\n",
            "Round 879: train_corr=+0.1581 valid_corr=+0.0279 trees/s=45.21\n",
            "Round 880: train_corr=+0.1581 valid_corr=+0.0279 trees/s=52.56\n",
            "Round 881: train_corr=+0.1582 valid_corr=+0.0279 trees/s=50.04\n",
            "Round 882: train_corr=+0.1583 valid_corr=+0.0279 trees/s=51.06\n",
            "Round 883: train_corr=+0.1584 valid_corr=+0.0279 trees/s=49.47\n",
            "Round 884: train_corr=+0.1584 valid_corr=+0.0279 trees/s=61.75\n",
            "Round 885: train_corr=+0.1585 valid_corr=+0.0279 trees/s=53.09\n",
            "Round 886: train_corr=+0.1586 valid_corr=+0.0279 trees/s=52.76\n",
            "Round 887: train_corr=+0.1587 valid_corr=+0.0279 trees/s=54.40\n",
            "Round 888: train_corr=+0.1588 valid_corr=+0.0279 trees/s=47.06\n",
            "Round 889: train_corr=+0.1589 valid_corr=+0.0279 trees/s=44.01\n",
            "Round 890: train_corr=+0.1590 valid_corr=+0.0279 trees/s=51.99\n",
            "Round 891: train_corr=+0.1590 valid_corr=+0.0279 trees/s=49.31\n",
            "Round 892: train_corr=+0.1592 valid_corr=+0.0279 trees/s=44.30\n",
            "Round 893: train_corr=+0.1593 valid_corr=+0.0279 trees/s=59.45\n",
            "Round 894: train_corr=+0.1594 valid_corr=+0.0279 trees/s=54.58\n",
            "Round 895: train_corr=+0.1594 valid_corr=+0.0279 trees/s=55.01\n",
            "Round 896: train_corr=+0.1594 valid_corr=+0.0279 trees/s=53.21\n",
            "Round 897: train_corr=+0.1595 valid_corr=+0.0279 trees/s=43.16\n",
            "Round 898: train_corr=+0.1596 valid_corr=+0.0279 trees/s=48.97\n",
            "Round 899: train_corr=+0.1597 valid_corr=+0.0279 trees/s=37.79\n",
            "Round 900: train_corr=+0.1597 valid_corr=+0.0282 trees/s=0.77\n",
            "Round 901: train_corr=+0.1598 valid_corr=+0.0282 trees/s=44.15\n",
            "Round 902: train_corr=+0.1599 valid_corr=+0.0282 trees/s=45.82\n",
            "Round 903: train_corr=+0.1599 valid_corr=+0.0282 trees/s=62.37\n",
            "Round 904: train_corr=+0.1600 valid_corr=+0.0282 trees/s=58.93\n",
            "Round 905: train_corr=+0.1601 valid_corr=+0.0282 trees/s=63.81\n",
            "Round 906: train_corr=+0.1601 valid_corr=+0.0282 trees/s=49.58\n",
            "Round 907: train_corr=+0.1602 valid_corr=+0.0282 trees/s=51.58\n",
            "Round 908: train_corr=+0.1602 valid_corr=+0.0282 trees/s=49.76\n",
            "Round 909: train_corr=+0.1603 valid_corr=+0.0282 trees/s=55.21\n",
            "Round 910: train_corr=+0.1603 valid_corr=+0.0282 trees/s=53.64\n",
            "Round 911: train_corr=+0.1604 valid_corr=+0.0282 trees/s=54.36\n",
            "Round 912: train_corr=+0.1605 valid_corr=+0.0282 trees/s=48.59\n",
            "Round 913: train_corr=+0.1606 valid_corr=+0.0282 trees/s=45.70\n",
            "Round 914: train_corr=+0.1606 valid_corr=+0.0282 trees/s=68.58\n",
            "Round 915: train_corr=+0.1607 valid_corr=+0.0282 trees/s=42.66\n",
            "Round 916: train_corr=+0.1608 valid_corr=+0.0282 trees/s=41.49\n",
            "Round 917: train_corr=+0.1609 valid_corr=+0.0282 trees/s=52.98\n",
            "Round 918: train_corr=+0.1609 valid_corr=+0.0282 trees/s=53.44\n",
            "Round 919: train_corr=+0.1610 valid_corr=+0.0282 trees/s=44.02\n",
            "Round 920: train_corr=+0.1611 valid_corr=+0.0282 trees/s=51.14\n",
            "Round 921: train_corr=+0.1612 valid_corr=+0.0282 trees/s=39.98\n",
            "Round 922: train_corr=+0.1612 valid_corr=+0.0282 trees/s=56.53\n",
            "Round 923: train_corr=+0.1613 valid_corr=+0.0282 trees/s=51.02\n",
            "Round 924: train_corr=+0.1614 valid_corr=+0.0282 trees/s=63.32\n",
            "Round 925: train_corr=+0.1615 valid_corr=+0.0282 trees/s=45.86\n",
            "Round 926: train_corr=+0.1616 valid_corr=+0.0282 trees/s=46.40\n",
            "Round 927: train_corr=+0.1616 valid_corr=+0.0282 trees/s=74.17\n",
            "Round 928: train_corr=+0.1617 valid_corr=+0.0282 trees/s=56.65\n",
            "Round 929: train_corr=+0.1617 valid_corr=+0.0282 trees/s=62.01\n",
            "Round 930: train_corr=+0.1618 valid_corr=+0.0282 trees/s=42.75\n",
            "Round 931: train_corr=+0.1619 valid_corr=+0.0282 trees/s=60.35\n",
            "Round 932: train_corr=+0.1620 valid_corr=+0.0282 trees/s=52.71\n",
            "Round 933: train_corr=+0.1620 valid_corr=+0.0282 trees/s=59.31\n",
            "Round 934: train_corr=+0.1621 valid_corr=+0.0282 trees/s=67.54\n",
            "Round 935: train_corr=+0.1621 valid_corr=+0.0282 trees/s=49.08\n",
            "Round 936: train_corr=+0.1623 valid_corr=+0.0282 trees/s=34.93\n",
            "Round 937: train_corr=+0.1623 valid_corr=+0.0282 trees/s=56.41\n",
            "Round 938: train_corr=+0.1624 valid_corr=+0.0282 trees/s=43.33\n",
            "Round 939: train_corr=+0.1625 valid_corr=+0.0282 trees/s=40.84\n",
            "Round 940: train_corr=+0.1626 valid_corr=+0.0282 trees/s=53.75\n",
            "Round 941: train_corr=+0.1626 valid_corr=+0.0282 trees/s=60.55\n",
            "Round 942: train_corr=+0.1627 valid_corr=+0.0282 trees/s=52.45\n",
            "Round 943: train_corr=+0.1628 valid_corr=+0.0282 trees/s=42.75\n",
            "Round 944: train_corr=+0.1629 valid_corr=+0.0282 trees/s=46.42\n",
            "Round 945: train_corr=+0.1630 valid_corr=+0.0282 trees/s=57.52\n",
            "Round 946: train_corr=+0.1630 valid_corr=+0.0282 trees/s=51.21\n",
            "Round 947: train_corr=+0.1631 valid_corr=+0.0282 trees/s=63.88\n",
            "Round 948: train_corr=+0.1632 valid_corr=+0.0282 trees/s=43.18\n",
            "Round 949: train_corr=+0.1633 valid_corr=+0.0282 trees/s=70.51\n",
            "Round 950: train_corr=+0.1634 valid_corr=+0.0282 trees/s=49.83\n",
            "Round 951: train_corr=+0.1634 valid_corr=+0.0282 trees/s=44.51\n",
            "Round 952: train_corr=+0.1635 valid_corr=+0.0282 trees/s=42.43\n",
            "Round 953: train_corr=+0.1636 valid_corr=+0.0282 trees/s=58.88\n",
            "Round 954: train_corr=+0.1637 valid_corr=+0.0282 trees/s=43.54\n",
            "Round 955: train_corr=+0.1637 valid_corr=+0.0282 trees/s=67.52\n",
            "Round 956: train_corr=+0.1638 valid_corr=+0.0282 trees/s=49.97\n",
            "Round 957: train_corr=+0.1638 valid_corr=+0.0282 trees/s=47.00\n",
            "Round 958: train_corr=+0.1639 valid_corr=+0.0282 trees/s=62.52\n",
            "Round 959: train_corr=+0.1640 valid_corr=+0.0282 trees/s=53.70\n",
            "Round 960: train_corr=+0.1640 valid_corr=+0.0282 trees/s=61.16\n",
            "Round 961: train_corr=+0.1640 valid_corr=+0.0282 trees/s=58.54\n",
            "Round 962: train_corr=+0.1641 valid_corr=+0.0282 trees/s=43.34\n",
            "Round 963: train_corr=+0.1642 valid_corr=+0.0282 trees/s=54.87\n",
            "Round 964: train_corr=+0.1642 valid_corr=+0.0282 trees/s=44.10\n",
            "Round 965: train_corr=+0.1643 valid_corr=+0.0282 trees/s=59.84\n",
            "Round 966: train_corr=+0.1643 valid_corr=+0.0282 trees/s=56.18\n",
            "Round 967: train_corr=+0.1644 valid_corr=+0.0282 trees/s=59.59\n",
            "Round 968: train_corr=+0.1644 valid_corr=+0.0282 trees/s=60.33\n",
            "Round 969: train_corr=+0.1645 valid_corr=+0.0282 trees/s=53.40\n",
            "Round 970: train_corr=+0.1646 valid_corr=+0.0282 trees/s=62.10\n",
            "Round 971: train_corr=+0.1647 valid_corr=+0.0282 trees/s=54.65\n",
            "Round 972: train_corr=+0.1647 valid_corr=+0.0282 trees/s=46.73\n",
            "Round 973: train_corr=+0.1648 valid_corr=+0.0282 trees/s=69.71\n",
            "Round 974: train_corr=+0.1648 valid_corr=+0.0282 trees/s=64.83\n",
            "Round 975: train_corr=+0.1649 valid_corr=+0.0282 trees/s=45.68\n",
            "Round 976: train_corr=+0.1650 valid_corr=+0.0282 trees/s=48.18\n",
            "Round 977: train_corr=+0.1651 valid_corr=+0.0282 trees/s=45.33\n",
            "Round 978: train_corr=+0.1652 valid_corr=+0.0282 trees/s=49.38\n",
            "Round 979: train_corr=+0.1652 valid_corr=+0.0282 trees/s=58.62\n",
            "Round 980: train_corr=+0.1654 valid_corr=+0.0282 trees/s=38.48\n",
            "Round 981: train_corr=+0.1654 valid_corr=+0.0282 trees/s=57.59\n",
            "Round 982: train_corr=+0.1655 valid_corr=+0.0282 trees/s=61.23\n",
            "Round 983: train_corr=+0.1656 valid_corr=+0.0282 trees/s=40.68\n",
            "Round 984: train_corr=+0.1657 valid_corr=+0.0282 trees/s=63.75\n",
            "Round 985: train_corr=+0.1657 valid_corr=+0.0282 trees/s=64.74\n",
            "Round 986: train_corr=+0.1658 valid_corr=+0.0282 trees/s=61.97\n",
            "Round 987: train_corr=+0.1658 valid_corr=+0.0282 trees/s=47.86\n",
            "Round 988: train_corr=+0.1658 valid_corr=+0.0282 trees/s=67.75\n",
            "Round 989: train_corr=+0.1659 valid_corr=+0.0282 trees/s=50.84\n",
            "Round 990: train_corr=+0.1659 valid_corr=+0.0282 trees/s=51.03\n",
            "Round 991: train_corr=+0.1660 valid_corr=+0.0282 trees/s=44.90\n",
            "Round 992: train_corr=+0.1661 valid_corr=+0.0282 trees/s=46.38\n",
            "Round 993: train_corr=+0.1662 valid_corr=+0.0282 trees/s=46.47\n",
            "Round 994: train_corr=+0.1662 valid_corr=+0.0282 trees/s=56.61\n",
            "Round 995: train_corr=+0.1663 valid_corr=+0.0282 trees/s=44.04\n",
            "Round 996: train_corr=+0.1664 valid_corr=+0.0282 trees/s=51.23\n",
            "Round 997: train_corr=+0.1664 valid_corr=+0.0282 trees/s=63.30\n",
            "Round 998: train_corr=+0.1665 valid_corr=+0.0282 trees/s=57.36\n",
            "Round 999: train_corr=+0.1666 valid_corr=+0.0282 trees/s=48.74\n",
            "Round 1000: train_corr=+0.1667 valid_corr=+0.0284 trees/s=0.70\n",
            "Finished training in 405.35 seconds\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     round  train_corr  trees_per_second  round_seconds  valid_corr\n",
              "0        1    0.020237         22.805561       0.350792         NaN\n",
              "1        2    0.023805         64.895602       0.123275         NaN\n",
              "2        3    0.027102         56.591467       0.141364         NaN\n",
              "3        4    0.031407         52.547137       0.152244         NaN\n",
              "4        5    0.032602         54.613361       0.146484         NaN\n",
              "..     ...         ...               ...            ...         ...\n",
              "995    996    0.166391         51.232018       0.156152    0.028216\n",
              "996    997    0.166439         63.297129       0.126388    0.028216\n",
              "997    998    0.166490         57.359727       0.139471    0.028216\n",
              "998    999    0.166600         48.743751       0.164124    0.028216\n",
              "999   1000    0.166740          0.695813      11.497341    0.028429\n",
              "\n",
              "[1000 rows x 5 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-20020ae5-11f0-4cba-8b82-14e4e47211ad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>round</th>\n",
              "      <th>train_corr</th>\n",
              "      <th>trees_per_second</th>\n",
              "      <th>round_seconds</th>\n",
              "      <th>valid_corr</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.020237</td>\n",
              "      <td>22.805561</td>\n",
              "      <td>0.350792</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>0.023805</td>\n",
              "      <td>64.895602</td>\n",
              "      <td>0.123275</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>0.027102</td>\n",
              "      <td>56.591467</td>\n",
              "      <td>0.141364</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0.031407</td>\n",
              "      <td>52.547137</td>\n",
              "      <td>0.152244</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>0.032602</td>\n",
              "      <td>54.613361</td>\n",
              "      <td>0.146484</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>996</td>\n",
              "      <td>0.166391</td>\n",
              "      <td>51.232018</td>\n",
              "      <td>0.156152</td>\n",
              "      <td>0.028216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>997</td>\n",
              "      <td>0.166439</td>\n",
              "      <td>63.297129</td>\n",
              "      <td>0.126388</td>\n",
              "      <td>0.028216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>998</td>\n",
              "      <td>0.166490</td>\n",
              "      <td>57.359727</td>\n",
              "      <td>0.139471</td>\n",
              "      <td>0.028216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>999</td>\n",
              "      <td>0.166600</td>\n",
              "      <td>48.743751</td>\n",
              "      <td>0.164124</td>\n",
              "      <td>0.028216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1000</td>\n",
              "      <td>0.166740</td>\n",
              "      <td>0.695813</td>\n",
              "      <td>11.497341</td>\n",
              "      <td>0.028429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows  5 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-20020ae5-11f0-4cba-8b82-14e4e47211ad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-20020ae5-11f0-4cba-8b82-14e4e47211ad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-20020ae5-11f0-4cba-8b82-14e4e47211ad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-d04a2c7c-5abc-4398-a8c3-d4a2aa611765\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d04a2c7c-5abc-4398-a8c3-d4a2aa611765')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-d04a2c7c-5abc-4398-a8c3-d4a2aa611765 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_6e786f03-9171-444c-b422-2fd3b96455ec\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('metrics_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_6e786f03-9171-444c-b422-2fd3b96455ec button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('metrics_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "metrics_df",
              "summary": "{\n  \"name\": \"metrics_df\",\n  \"rows\": 1000,\n  \"fields\": [\n    {\n      \"column\": \"round\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 288,\n        \"min\": 1,\n        \"max\": 1000,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          522,\n          738,\n          741\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_corr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.03331190294906961,\n        \"min\": 0.020237499848008156,\n        \"max\": 0.1667403280735016,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.12877364456653595,\n          0.14738275110721588,\n          0.1475779265165329\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"trees_per_second\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 9.762076607492954,\n        \"min\": 0.6958130627136584,\n        \"max\": 74.88150399820027,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          54.15125665784082,\n          61.41301647765991,\n          43.74995642127153\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"round_seconds\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7028795351249465,\n        \"min\": 0.10683546099971863,\n        \"max\": 11.497340922000149,\n        \"num_unique_values\": 1000,\n        \"samples\": [\n          0.14773433699883753,\n          0.1302655439976661,\n          0.18285732499862206\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"valid_corr\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.002865329876549756,\n        \"min\": 0.019294921308755875,\n        \"max\": 0.028428811579942703,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.028216222301125526,\n          0.022160109132528305,\n          0.027195915579795837\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "from packboost.booster import PackBoost\n",
        "from packboost.config import PackBoostConfig\n",
        "from packboost import backends\n",
        "from time import perf_counter\n",
        "import pandas as pd\n",
        "\n",
        "print('CUDA backend available:', backends.cuda_available())\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "config = PackBoostConfig(\n",
        "    pack_size=8,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.07,\n",
        "    lambda_l2=1e-6,\n",
        "    lambda_dro=0.0,\n",
        "    direction_weight=0.0,\n",
        "    min_samples_leaf=20,\n",
        "    max_bins=5,\n",
        "    k_cuts=4,\n",
        "    cut_selection='mass',\n",
        "    device=str(DEVICE),\n",
        "    prebinned=True,\n",
        "    layer_feature_fraction=0.0538,\n",
        "    random_state=42,\n",
        "    histogram_mode='subtract'\n",
        ")\n",
        "\n",
        "round_logs = []\n",
        "\n",
        "def log_round(idx: int, metrics: dict[str, float]) -> None:\n",
        "    print(f\"Round {metrics['round']:>3}: train_corr={metrics['train_corr']:+.4f} \"\n",
        "          f\"valid_corr={metrics.get('valid_corr', float('nan')):+.4f} \"\n",
        "          f\"trees/s={metrics['trees_per_second']:.2f}\")\n",
        "    round_logs.append(metrics)\n",
        "\n",
        "booster = PackBoost(config)\n",
        "start = perf_counter()\n",
        "booster.fit(\n",
        "    Xt,\n",
        "    yt,\n",
        "    None, #Et\n",
        "    num_rounds=1000,\n",
        "    eval_sets=[('valid', Xv, Yv, None)],\n",
        "    round_callback=log_round,\n",
        ")\n",
        "elapsed = perf_counter() - start\n",
        "print(f\"Finished training in {elapsed:.2f} seconds\")\n",
        "\n",
        "metrics_df = pd.DataFrame(round_logs)\n",
        "metrics_df\n"
      ],
      "id": "train"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKddr1wZpxy9"
      },
      "source": [
        "## Plot per-round correlation\n"
      ],
      "id": "XKddr1wZpxy9"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "plot",
        "outputId": "cfd8170c-17aa-490f-9e20-9a60366300f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "if not metrics_df.empty:\n",
        "    ax = metrics_df.plot(x='round', y=['train_corr', 'valid_corr'], marker='o')\n",
        "    ax.set_ylabel('Correlation')\n",
        "    ax.set_title('PackBoost correlation per round')\n",
        "    ax.grid(True)\n",
        "    plt.show()\n"
      ],
      "id": "plot"
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_parquet(valid_path)\n",
        "\n",
        "preds = booster.predict(test[FEATURES].astype(np.int8).values)\n",
        "\n",
        "subm = pd.DataFrame()\n",
        "subm.index = test.index\n",
        "subm[\"prediction\"] = preds\n",
        "subm[['prediction']].rank(pct=True).to_csv(\"PackBoost1.csv\")"
      ],
      "metadata": {
        "id": "s3SZSRwZZMUX"
      },
      "id": "s3SZSRwZZMUX",
      "execution_count": 8,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}