{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PackBoost Numerai Demo\n",
    "\n",
    "This notebook downloads a slice of the Numerai dataset, bins features, and\n",
    "trains PackBoost on CPU and (optionally) GPU. It is designed for Google\n",
    "Colab â€“ adjust the installation cell if you prefer a local environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Install dependencies (edit REPO_URL if you forked the project)\n",
    "REPO_URL = \"https://github.com/pranshubahadur/PackBoost.git\"  # TODO: update if needed\n",
    "\n",
    "!pip install -q numerapi pandas~=2.2 pyarrow~=15.0 catboost lightgbm xgboost\n",
    "# Install CuPy for GPU acceleration. Pick the wheel matching your CUDA version.\n",
    "try:\n",
    "    import cupy  # noqa: F401\n",
    "except ImportError:\n",
    "    !pip install -q cupy-cuda12x\n",
    "\n",
    "# Clone PackBoost and install in editable mode\n",
    "import pathlib, sys\n",
    "work_dir = pathlib.Path.cwd()\n",
    "if not (work_dir / \"PackBoost\").exists():\n",
    "    !git clone {REPO_URL} PackBoost\n",
    "!pip install -q -e PackBoost\n",
    "sys.path.append(str(work_dir / \"PackBoost\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Numerai data\n",
    "This cell pulls the v5.0 training and validation data. Set your API keys if\n",
    "you plan to upload diagnostics at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numerapi import NumerAPI\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"numerai-data\")\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "napi = NumerAPI()\n",
    "napi.download_dataset('v5.0/train.parquet', str(DATA_DIR / 'train.parquet'))\n",
    "napi.download_dataset('v5.0/validation.parquet', str(DATA_DIR / 'validation.parquet'))\n",
    "napi.download_dataset('v5.0/features.json', str(DATA_DIR / 'features.json'))\n",
    "\n",
    "train_df = pd.read_parquet(DATA_DIR / 'train.parquet')\n",
    "valid_df = pd.read_parquet(DATA_DIR / 'validation.parquet')\n",
    "features_json = pd.read_json(DATA_DIR / 'features.json')\n",
    "feature_list = features_json['feature_sets']['all']\n",
    "\n",
    "# keep recent eras for the demo to speed things up\n",
    "train_df = train_df[train_df['era'] >= train_df['era'].max() - 10]\n",
    "valid_df = valid_df[valid_df['era'] >= valid_df['era'].max() - 4]\n",
    "\n",
    "print('Train shape:', train_df.shape)\n",
    "print('Valid shape:', valid_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning and feature prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from packboost.utils.binning import quantile_binning, apply_binning\n",
    "\n",
    "X_train = train_df[feature_list].astype(np.float32).values\n",
    "y_train = train_df['target'].astype(np.float32).values\n",
    "era_train = train_df['era'].astype(np.int32).values\n",
    "\n",
    "X_valid = valid_df[feature_list].astype(np.float32).values\n",
    "y_valid = valid_df['target'].astype(np.float32).values\n",
    "era_valid = valid_df['era'].astype(np.int32).values\n",
    "\n",
    "X_train_binned, bin_edges = quantile_binning(X_train, max_bins=128, random_state=42)\n",
    "X_valid_binned = apply_binning(X_valid, bin_edges)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train PackBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from packboost import PackBoost, PackBoostConfig\n",
    "from packboost.gpu import has_cuda\n",
    "from sklearn.metrics import r2_score\n",
    "import time\n",
    "\n",
    "device = 'cuda' if has_cuda() else 'cpu'\n",
    "config = PackBoostConfig(\n",
    "    pack_size=8,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.05,\n",
    "    lambda_l2=1.0,\n",
    "    lambda_dro=0.5,\n",
    "    max_bins=128,\n",
    "    min_samples_leaf=32,\n",
    "    random_state=42,\n",
    "    layer_feature_fraction=0.5,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "booster = PackBoost(config)\n",
    "start = time.perf_counter()\n",
    "booster.fit(X_train, y_train, era_train, num_rounds=20)\n",
    "fit_time = time.perf_counter() - start\n",
    "\n",
    "start = time.perf_counter()\n",
    "preds = booster.predict(X_valid)\n",
    "pred_time = time.perf_counter() - start\n",
    "\n",
    "print(f'Device: {device}')\n",
    "print(f'Fit time: {fit_time:.2f}s')\n",
    "print(f'Predict time: {pred_time:.2f}s')\n",
    "print(f'Validation R^2: {r2_score(y_valid, preds):.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload diagnostics (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your credentials to upload to Numerai\n",
    "# napi = NumerAPI(public_id='PUBLIC', secret_key='SECRET')\n",
    "# submission = pd.DataFrame({'prediction': preds}).clip(0, 1)\n",
    "# submission.to_csv('packboost_preds.csv', index=False)\n",
    "# napi.upload_diagnostics('packboost_preds.csv', model_id='YOUR_MODEL_ID')\n",
    "print('Diagnostics upload skipped.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
