{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40pDPuy1pxy6"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Pranshu-Bahadur/PackBoost/blob/main/notebooks/numerai_gpu_demo.ipynb)\n"
      ],
      "id": "40pDPuy1pxy6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tov5riA0pxy7"
      },
      "source": [
        "# PackBoost Numerai GPU Demo\n",
        "\n",
        "Train PackBoost on Numerai v5.0 with CUDA frontier enabled, bucket eras into user-defined\n",
        "groups, and monitor per-round train/validation correlation as well as trees-per-second.\n"
      ],
      "id": "Tov5riA0pxy7"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"]=\"1\"\n",
        "os.environ[\"TORCH_USE_CUDA_DSA\"]=\"1\""
      ],
      "metadata": {
        "id": "-DQ6KkJwx_AW"
      },
      "id": "-DQ6KkJwx_AW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PackBoost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUrjwmgMzNWt",
        "outputId": "dcc0bcbe-68e8-432f-b4e0-4fbdef9147ab"
      },
      "id": "kUrjwmgMzNWt",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PackBoost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PACKBOOST_DISABLE_PACK_PREDICT=0 python examples/hull_benchmark.py  --data datasets/train.csv --era-size 1 --holdout-eras 180 --to-signal-mult 400 --lastn 180  --impute ffill --target-col forward_excess --fx-groups '' --des-off --efb-disable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOvqe4nOzQfe",
        "outputId": "1ef6a0ef-e2aa-4824-d8de-fd13ed0d5b83"
      },
      "id": "YOvqe4nOzQfe",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data: rows=8,990, base features=94, engineered+base=94, unique raw eras=8990, unique bucketed eras=1\n",
            "Era mode: DES-OFF (single era) | Target: market_forward_excess_returns\n",
            "Engineered on groups: []\n",
            "Impute: ffill\n",
            "DES-OFF row split: train=8810, test=180 (holdout_frac=0.02)\n",
            "EFB (PackBoost): DISABLED.\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "\n",
            "Model         Fit (s)   Predict (s)     R^2   EraMeanCorr   EraSharpe   LastNSharpe   HullPenalized\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "PackBoost       1.955        0.031  -0.0089      -0.0811         nan        0.3000        0.1771\n",
            "XGBoost         0.690        0.002  -0.3028      -0.0113         nan        0.2162        0.0251\n",
            "LightGBM        0.895        0.003  -0.1286       0.0882         nan        0.4361        0.1788\n",
            "CatBoost        3.147        0.002  -0.1232      -0.0536         nan       -0.1622       -0.0540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python examples/hull_benchmark.py  --data datasets/train.csv --era-size 180 --holdout-eras 1 --to-signal-mult 1600 --lastn 180  --impute ffill --target-col forward_excess"
      ],
      "metadata": {
        "id": "Y3OmzFcYIc1D",
        "outputId": "c23c4c26-3f71-4f30-9ca8-4fa1c46e5eec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Y3OmzFcYIc1D",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:169: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_lag1\"] = s.shift(1)\n",
            "/content/PackBoost/examples/hull_benchmark.py:175: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_z{window_z}\"] = z.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:180: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[c + f\"_mom{window_mom}\"] = mom.astype(np.float32)\n",
            "/content/PackBoost/examples/hull_benchmark.py:512: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
            "  df[\"_era_bucket\"] = bucketize_era(df[era_col_raw], size=int(args.era_size))\n",
            "Data: rows=8,990, base features=94, engineered+base=376, unique raw eras=8990, unique bucketed eras=50\n",
            "Era mode: bucketed (size=180) | Target: market_forward_excess_returns\n",
            "Engineered on groups: ['M', 'E', 'I', 'P', 'V', 'S', 'MOM', 'D']\n",
            "Impute: ffill\n",
            "EFB (PackBoost): bundled 376 cols into 104 bundles. New PB dims: train=104, test=104\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "/content/PackBoost/examples/hull_benchmark.py:73: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  cors = df.groupby(\"era\", sort=False).apply(\n",
            "\n",
            "Model         Fit (s)   Predict (s)     R^2   EraMeanCorr   EraSharpe   LastNSharpe   HullPenalized\n",
            "--------------------------------------------------------------------------------------------------------------\n",
            "PackBoost      17.646        0.041  -0.0037       0.0373         nan       -0.1010       -0.0463\n",
            "XGBoost         1.848        0.002  -0.1595       0.0085         nan        0.8433        0.4192\n",
            "LightGBM        2.586        0.004  -0.1359       0.0590         nan        0.0250       -0.0300\n",
            "CatBoost        8.970        0.003  -0.1128      -0.0006         nan        0.1056       -0.0145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i9dHovf3IW5v"
      },
      "id": "i9dHovf3IW5v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P3Ou_wN1t4n",
        "outputId": "4da4e56c-2582-4993-e28a-9cc2edfaca85"
      },
      "id": "8P3Ou_wN1t4n",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!PACKBOOST_DISABLE_PACK_PREDICT=0 python examples/synthetic_benchmark.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrzNh8y41oTg",
        "outputId": "5893bb07-c407-408e-d964-d9ae09bc701f"
      },
      "id": "KrzNh8y41oTg",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model         Fit (s)   Predict (s)   R^2\n",
            "--------------------------------------------\n",
            "PackBoost       5.109        0.049  0.9302\n",
            "XGBoost         0.843        0.004  0.9619\n",
            "LightGBM        0.453        0.008  0.9413\n",
            "CatBoost        1.131        0.002  0.9915\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import packboost._backend as B\n",
        "for name in (\"predict_bins_cuda\",\n",
        "             \"predict_pack_cuda\",\n",
        "             \"partition_frontier_cuda\",\n",
        "             \"find_best_splits_batched_cuda\"):\n",
        "    print(name, type(getattr(B, name, None)))\n"
      ],
      "metadata": {
        "id": "vRQ85W1Vs8kK",
        "outputId": "d2ee83b6-aeb9-46de-efb6-6de4723e23be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "vRQ85W1Vs8kK",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predict_bins_cuda <class 'builtin_function_or_method'>\n",
            "predict_pack_cuda <class 'builtin_function_or_method'>\n",
            "partition_frontier_cuda <class 'builtin_function_or_method'>\n",
            "find_best_splits_batched_cuda <class 'builtin_function_or_method'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PackBoost"
      ],
      "metadata": {
        "id": "3F49uVr7QWal",
        "outputId": "0469030e-6ecb-4f50-f4e1-431403084344",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3F49uVr7QWal",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PackBoost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf build"
      ],
      "metadata": {
        "id": "8rDfN8QHQdzf"
      },
      "id": "8rDfN8QHQdzf",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find packboost -name \"_backend*.so\" -delete"
      ],
      "metadata": {
        "id": "DgD7MzIOQmWz"
      },
      "id": "DgD7MzIOQmWz",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python setup_native.py build_ext --inplace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8lBc4Lzkts-m",
        "outputId": "e38dc897-6737-40bb-e562-3273b72838c6"
      },
      "id": "8lBc4Lzkts-m",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[setup] compiling CUDA backend: /usr/local/cuda/bin/nvcc -std=c++17 -O3 -Xcompiler -fPIC -c /content/PackBoost/packboost/backends/cuda/frontier_cuda.cu -o /content/PackBoost/build/temp/backend_cuda.o -gencode arch=compute_70,code=sm_70 -gencode arch=compute_70,code=compute_70 -gencode arch=compute_75,code=sm_75 -gencode arch=compute_75,code=compute_75 -gencode arch=compute_80,code=sm_80 -gencode arch=compute_80,code=compute_80 -gencode arch=compute_86,code=sm_86 -gencode arch=compute_86,code=compute_86 -I /usr/local/lib/python3.12/dist-packages/pybind11/include -I /usr/local/lib/python3.12/dist-packages/pybind11/include -I /usr/include/python3.12 -I /usr/local/lib/python3.12/dist-packages/numpy/_core/include -I packboost/backends -DPACKBOOST_ENABLE_CUDA=1\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(270)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"lane\"\u001b[0m was declared but never referenced\n",
            "    const int lane = threadIdx.x & (WARP_SIZE-1);\n",
            "              ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(78)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(133)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(270)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"lane\"\u001b[0m was declared but never referenced\n",
            "    const int lane = threadIdx.x & (WARP_SIZE-1);\n",
            "              ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(78)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(133)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(270)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"lane\"\u001b[0m was declared but never referenced\n",
            "    const int lane = threadIdx.x & (WARP_SIZE-1);\n",
            "              ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(78)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(133)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(270)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"lane\"\u001b[0m was declared but never referenced\n",
            "    const int lane = threadIdx.x & (WARP_SIZE-1);\n",
            "              ^\n",
            "\n",
            "\u001b[01;36m\u001b[0m\u001b[01;36mRemark\u001b[0m: The warnings can be suppressed with \"-diag-suppress <warning-number>\"\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(78)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "\u001b[01m\u001b[0m\u001b[01m/content/PackBoost/packboost/backends/cuda/frontier_cuda.cu(133)\u001b[0m: \u001b[01;35mwarning\u001b[0m #177-D: variable \u001b[01m\"node_base\"\u001b[0m was declared but never referenced\n",
            "      const int32_t node_base = node_row_splits[n];\n",
            "                    ^\n",
            "\n",
            "running build_ext\n",
            "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -I/usr/include/python3.12 -c flagcheck.cpp -o flagcheck.o -std=c++17\n",
            "building 'packboost._backend' extension\n",
            "creating build/temp.linux-x86_64-cpython-312/packboost/backends\n",
            "creating build/temp.linux-x86_64-cpython-312/packboost/backends/cpu\n",
            "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DPACKBOOST_ENABLE_CUDA=1 -Ipackboost/backends -I/usr/local/lib/python3.12/dist-packages/numpy/_core/include -I/usr/local/lib/python3.12/dist-packages/pybind11/include -I/usr/include/python3.12 -c packboost/backends/bindings.cpp -o build/temp.linux-x86_64-cpython-312/packboost/backends/bindings.o -std=c++17 -fvisibility=hidden -g0 -O3 -std=c++17 -fvisibility=hidden -fopenmp\n",
            "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -fPIC -DPACKBOOST_ENABLE_CUDA=1 -Ipackboost/backends -I/usr/local/lib/python3.12/dist-packages/numpy/_core/include -I/usr/local/lib/python3.12/dist-packages/pybind11/include -I/usr/include/python3.12 -c packboost/backends/cpu/frontier_cpu.cpp -o build/temp.linux-x86_64-cpython-312/packboost/backends/cpu/frontier_cpu.o -std=c++17 -fvisibility=hidden -g0 -O3 -std=c++17 -fvisibility=hidden -fopenmp\n",
            "\u001b[01m\u001b[Kpackboost/backends/cpu/frontier_cpu.cpp:\u001b[m\u001b[K In function \u001b[01m\u001b[Kpybind11::tuple {anonymous}::find_best_splits_batched_cpu(pybind11::array_t<signed char, 17>, pybind11::array_t<float, 17>, const pybind11::list&, pybind11::array_t<int, 17>, int, int, const string&, float, float, float, float, int)\u001b[m\u001b[K:\n",
            "\u001b[01m\u001b[Kpackboost/backends/cpu/frontier_cpu.cpp:148:35:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: \u001b[01m\u001b[Kpybind11::ssize_t\u001b[m\u001b[K {aka \u001b[01m\u001b[Klong int\u001b[m\u001b[K} and \u001b[01m\u001b[Kpybind11::size_t\u001b[m\u001b[K {aka \u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  148 |     for (py::ssize_t idx = 0; \u001b[01;35m\u001b[Kidx < nodes_era_rows.size()\u001b[m\u001b[K; ++idx) {\n",
            "      |                               \u001b[01;35m\u001b[K~~~~^~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "\u001b[01m\u001b[Kpackboost/backends/cpu/frontier_cpu.cpp:163:35:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: \u001b[01m\u001b[Kpybind11::ssize_t\u001b[m\u001b[K {aka \u001b[01m\u001b[Klong int\u001b[m\u001b[K} and \u001b[01m\u001b[Kpybind11::size_t\u001b[m\u001b[K {aka \u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K} [\u001b[01;35m\u001b[K\u001b]8;;https://gcc.gnu.org/onlinedocs/gcc/Warning-Options.html#index-Wsign-compare\u0007-Wsign-compare\u001b]8;;\u0007\u001b[m\u001b[K]\n",
            "  163 |         for (py::ssize_t e = 0; \u001b[01;35m\u001b[Ke < eras.size()\u001b[m\u001b[K; ++e) {\n",
            "      |                                 \u001b[01;35m\u001b[K~~^~~~~~~~~~~~~\u001b[m\u001b[K\n",
            "creating build/lib.linux-x86_64-cpython-312/packboost\n",
            "x86_64-linux-gnu-g++ -fno-strict-overflow -Wsign-compare -DNDEBUG -g -O2 -Wall -g -fstack-protector-strong -Wformat -Werror=format-security -g -fwrapv -O2 -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -g -fwrapv -O2 build/temp.linux-x86_64-cpython-312/packboost/backends/bindings.o build/temp.linux-x86_64-cpython-312/packboost/backends/cpu/frontier_cpu.o /content/PackBoost/build/temp/backend_cuda.o -L/usr/local/cuda-12.5/lib64 -L/usr/lib/x86_64-linux-gnu -Wl,--enable-new-dtags,-rpath,/usr/local/cuda-12.5/lib64 -lcudart -o build/lib.linux-x86_64-cpython-312/packboost/_backend.cpython-312-x86_64-linux-gnu.so -fopenmp\n",
            "copying build/lib.linux-x86_64-cpython-312/packboost/_backend.cpython-312-x86_64-linux-gnu.so -> packboost\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybind11"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVCGLY3Fp2GJ",
        "outputId": "f9c31a61-6576-43c4-e5ed-97b59fedb48f"
      },
      "id": "RVCGLY3Fp2GJ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybind11\n",
            "  Downloading pybind11-3.0.1-py3-none-any.whl.metadata (10.0 kB)\n",
            "Downloading pybind11-3.0.1-py3-none-any.whl (293 kB)\n",
            "\u001b[?25l   \u001b[90m\u001b[0m \u001b[32m0.0/293.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m\u001b[0m \u001b[32m293.6/293.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pybind11\n",
            "Successfully installed pybind11-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-packboost",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc763307-1796-4bed-abf7-7f40cc91cfcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PackBoost ready.\n"
          ]
        }
      ],
      "source": [
        "#@title Install PackBoost and dependencies\n",
        "REPO_URL = \"https://github.com/Pranshu-Bahadur/PackBoost.git\"  # change if using a fork\n",
        "\n",
        "import subprocess, sys, os\n",
        "\n",
        "if not os.path.exists('PackBoost'):\n",
        "    subprocess.run(['git', 'clone', REPO_URL, 'PackBoost'], check=True)\n",
        "\n",
        "os.chdir('PackBoost')\n",
        "\n",
        "# Build native extensions (enables CUDA frontier when nvcc is present)\n",
        "subprocess.run([sys.executable, 'setup_native.py', 'build_ext', '--inplace'], check=True)\n",
        "\n",
        "# Install in editable mode so notebooks can import packboost\n",
        "subprocess.run([sys.executable, '-m', 'pip', 'install', '-e', '.'], check=True)\n",
        "\n",
        "print('PackBoost ready.')\n"
      ],
      "id": "install-packboost"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67b4rPc_pxy8"
      },
      "source": [
        "## Download Numerai data\n",
        "\n",
        "This cell pulls the v5.0 training and validation parquet files. Provide keys if you plan to\n",
        "upload diagnostics later.\n"
      ],
      "id": "67b4rPc_pxy8"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numerapi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFiYERJ8rBaM",
        "outputId": "5b1351ab-43b5-44b4-b973-949f7458ff7f"
      },
      "id": "bFiYERJ8rBaM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numerapi in /usr/local/lib/python3.12/dist-packages (2.20.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from numerapi) (2.32.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.12/dist-packages (from numerapi) (2025.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from numerapi) (2.9.0.post0)\n",
            "Requirement already satisfied: tqdm>=4.29.1 in /usr/local/lib/python3.12/dist-packages (from numerapi) (4.67.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.12/dist-packages (from numerapi) (8.2.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from numerapi) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.0->numerapi) (2.0.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.1.0->numerapi) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil->numerapi) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->numerapi) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->numerapi) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->numerapi) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->numerapi) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "download-data"
      },
      "outputs": [],
      "source": [
        "from numerapi import NumerAPI\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "DATA_VERSION = 'v5.0'\n",
        "DATA_DIR = Path('numerai-data')\n",
        "DATA_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "napi = NumerAPI()\n",
        "\n",
        "feature_path = DATA_DIR / 'features.json'\n",
        "train_path = DATA_DIR / 'train.parquet'\n",
        "valid_path = DATA_DIR / 'validation.parquet'\n",
        "\n",
        "if not feature_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/features.json\", str(feature_path))\n",
        "if not train_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/train.parquet\", str(train_path))\n",
        "if not valid_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\", str(valid_path))\n",
        "\n",
        "with feature_path.open('r', encoding='utf-8') as fh:\n",
        "    FEATURES = json.load(fh)['feature_sets']['all']\n"
      ],
      "id": "download-data"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW4fbP13pxy8"
      },
      "source": [
        "## Preprocess\n",
        "\n",
        "* Convert targets to float32 and drop rows where the target is NaN.\n",
        "* Bucket consecutive eras into groups of configurable size (default 64).\n",
        "* Leave features as int8 bins so we can set `prebinned=True`.\n"
      ],
      "id": "SW4fbP13pxy8"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "preprocess",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e12cd5-edb5-4bc1-f344-f99c092ebbdc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "33"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "ERA_BUCKET_SIZE = 512  # feel free to tweak\n",
        "\n",
        "train_df = pd.read_parquet(train_path)\n",
        "#train_df = train_df.dropna(subset=['target']).reset_index(drop=True)\n",
        "train_df['era'] = train_df['era'].astype(np.int16)\n",
        "train_df['era_bucket'] = (train_df['era'] // ERA_BUCKET_SIZE).astype(np.int16)\n",
        "\n",
        "Xt = train_df[FEATURES].astype(np.int8).values\n",
        "yt = train_df['target'].astype(np.float32).values\n",
        "Et = train_df['era_bucket'].to_numpy(np.int16)\n",
        "\n",
        "valid_df = pd.read_parquet(valid_path)\n",
        "valid_df = valid_df.dropna(subset=['target'])#.reset_index(drop=True)\n",
        "valid_df['era'] = valid_df['era'].astype(np.int16)\n",
        "valid_df['era_bucket'] = (valid_df['era'] // ERA_BUCKET_SIZE).astype(np.int16)\n",
        "\n",
        "Xv = valid_df[FEATURES].astype(np.int8).values\n",
        "Yv = valid_df['target'].astype(np.float32).values\n",
        "Ev = valid_df['era_bucket'].to_numpy(np.int16)\n",
        "\n",
        "del train_df\n",
        "gc.collect()\n"
      ],
      "id": "preprocess"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8uMLsMZpxy8"
      },
      "source": [
        "## Train PackBoost with CUDA frontier\n",
        "\n",
        "The training loop logs metrics each round (train/validation correlation and trees per second).\n"
      ],
      "id": "Y8uMLsMZpxy8"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PACKBOOST_DISABLE_PACK_PREDICT\"] = \"0\"\n",
        "os.environ[\"PACKBOOST_EVAL_EVERY\"] = \"100\"\n"
      ],
      "metadata": {
        "id": "t7OfR_pCloEI"
      },
      "id": "t7OfR_pCloEI",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa489a98-04d5-4137-bd69-95650a57db85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA backend available: True\n",
            "Round   1: train_corr=+0.0169 valid_corr=+nan trees/s=22.56\n",
            "Round   2: train_corr=+0.0221 valid_corr=+nan trees/s=65.49\n",
            "Round   3: train_corr=+0.0259 valid_corr=+nan trees/s=54.66\n",
            "Round   4: train_corr=+0.0291 valid_corr=+nan trees/s=56.79\n",
            "Round   5: train_corr=+0.0291 valid_corr=+nan trees/s=60.08\n",
            "Round   6: train_corr=+0.0289 valid_corr=+nan trees/s=54.54\n",
            "Round   7: train_corr=+0.0315 valid_corr=+nan trees/s=60.29\n",
            "Round   8: train_corr=+0.0335 valid_corr=+nan trees/s=67.48\n",
            "Round   9: train_corr=+0.0347 valid_corr=+nan trees/s=61.97\n",
            "Round  10: train_corr=+0.0363 valid_corr=+nan trees/s=56.07\n",
            "Round  11: train_corr=+0.0370 valid_corr=+nan trees/s=66.43\n",
            "Round  12: train_corr=+0.0378 valid_corr=+nan trees/s=64.69\n",
            "Round  13: train_corr=+0.0385 valid_corr=+nan trees/s=73.92\n",
            "Round  14: train_corr=+0.0396 valid_corr=+nan trees/s=57.62\n",
            "Round  15: train_corr=+0.0402 valid_corr=+nan trees/s=73.34\n",
            "Round  16: train_corr=+0.0402 valid_corr=+nan trees/s=54.32\n",
            "Round  17: train_corr=+0.0408 valid_corr=+nan trees/s=49.15\n",
            "Round  18: train_corr=+0.0420 valid_corr=+nan trees/s=63.00\n",
            "Round  19: train_corr=+0.0418 valid_corr=+nan trees/s=57.97\n",
            "Round  20: train_corr=+0.0423 valid_corr=+nan trees/s=62.96\n",
            "Round  21: train_corr=+0.0431 valid_corr=+nan trees/s=61.00\n",
            "Round  22: train_corr=+0.0433 valid_corr=+nan trees/s=56.92\n",
            "Round  23: train_corr=+0.0436 valid_corr=+nan trees/s=65.85\n",
            "Round  24: train_corr=+0.0443 valid_corr=+nan trees/s=51.92\n",
            "Round  25: train_corr=+0.0448 valid_corr=+nan trees/s=71.91\n",
            "Round  26: train_corr=+0.0448 valid_corr=+nan trees/s=58.13\n",
            "Round  27: train_corr=+0.0452 valid_corr=+nan trees/s=52.30\n",
            "Round  28: train_corr=+0.0453 valid_corr=+nan trees/s=50.88\n",
            "Round  29: train_corr=+0.0457 valid_corr=+nan trees/s=72.86\n",
            "Round  30: train_corr=+0.0461 valid_corr=+nan trees/s=50.88\n",
            "Round  31: train_corr=+0.0466 valid_corr=+nan trees/s=64.29\n",
            "Round  32: train_corr=+0.0466 valid_corr=+nan trees/s=55.50\n",
            "Round  33: train_corr=+0.0469 valid_corr=+nan trees/s=49.10\n",
            "Round  34: train_corr=+0.0473 valid_corr=+nan trees/s=42.16\n",
            "Round  35: train_corr=+0.0480 valid_corr=+nan trees/s=52.82\n",
            "Round  36: train_corr=+0.0480 valid_corr=+nan trees/s=47.14\n",
            "Round  37: train_corr=+0.0486 valid_corr=+nan trees/s=53.33\n",
            "Round  38: train_corr=+0.0486 valid_corr=+nan trees/s=68.84\n",
            "Round  39: train_corr=+0.0487 valid_corr=+nan trees/s=68.35\n",
            "Round  40: train_corr=+0.0491 valid_corr=+nan trees/s=69.92\n",
            "Round  41: train_corr=+0.0495 valid_corr=+nan trees/s=52.89\n",
            "Round  42: train_corr=+0.0496 valid_corr=+nan trees/s=45.58\n",
            "Round  43: train_corr=+0.0499 valid_corr=+nan trees/s=58.60\n",
            "Round  44: train_corr=+0.0500 valid_corr=+nan trees/s=47.07\n",
            "Round  45: train_corr=+0.0502 valid_corr=+nan trees/s=73.30\n",
            "Round  46: train_corr=+0.0507 valid_corr=+nan trees/s=52.80\n",
            "Round  47: train_corr=+0.0507 valid_corr=+nan trees/s=61.57\n",
            "Round  48: train_corr=+0.0512 valid_corr=+nan trees/s=47.52\n",
            "Round  49: train_corr=+0.0516 valid_corr=+nan trees/s=58.36\n",
            "Round  50: train_corr=+0.0518 valid_corr=+nan trees/s=51.67\n",
            "Round  51: train_corr=+0.0520 valid_corr=+nan trees/s=43.85\n",
            "Round  52: train_corr=+0.0522 valid_corr=+nan trees/s=62.80\n",
            "Round  53: train_corr=+0.0526 valid_corr=+nan trees/s=49.08\n",
            "Round  54: train_corr=+0.0529 valid_corr=+nan trees/s=65.52\n",
            "Round  55: train_corr=+0.0532 valid_corr=+nan trees/s=71.27\n",
            "Round  56: train_corr=+0.0535 valid_corr=+nan trees/s=42.46\n",
            "Round  57: train_corr=+0.0535 valid_corr=+nan trees/s=60.22\n",
            "Round  58: train_corr=+0.0536 valid_corr=+nan trees/s=47.38\n",
            "Round  59: train_corr=+0.0539 valid_corr=+nan trees/s=58.42\n",
            "Round  60: train_corr=+0.0539 valid_corr=+nan trees/s=44.34\n",
            "Round  61: train_corr=+0.0543 valid_corr=+nan trees/s=65.14\n",
            "Round  62: train_corr=+0.0545 valid_corr=+nan trees/s=74.83\n",
            "Round  63: train_corr=+0.0549 valid_corr=+nan trees/s=68.41\n",
            "Round  64: train_corr=+0.0553 valid_corr=+nan trees/s=48.18\n",
            "Round  65: train_corr=+0.0558 valid_corr=+nan trees/s=41.18\n",
            "Round  66: train_corr=+0.0562 valid_corr=+nan trees/s=51.37\n",
            "Round  67: train_corr=+0.0564 valid_corr=+nan trees/s=41.97\n",
            "Round  68: train_corr=+0.0567 valid_corr=+nan trees/s=57.87\n",
            "Round  69: train_corr=+0.0572 valid_corr=+nan trees/s=59.61\n",
            "Round  70: train_corr=+0.0572 valid_corr=+nan trees/s=62.67\n",
            "Round  71: train_corr=+0.0573 valid_corr=+nan trees/s=59.42\n",
            "Round  72: train_corr=+0.0573 valid_corr=+nan trees/s=61.34\n",
            "Round  73: train_corr=+0.0574 valid_corr=+nan trees/s=65.32\n",
            "Round  74: train_corr=+0.0577 valid_corr=+nan trees/s=48.26\n",
            "Round  75: train_corr=+0.0579 valid_corr=+nan trees/s=40.94\n",
            "Round  76: train_corr=+0.0581 valid_corr=+nan trees/s=53.36\n",
            "Round  77: train_corr=+0.0584 valid_corr=+nan trees/s=52.32\n",
            "Round  78: train_corr=+0.0586 valid_corr=+nan trees/s=50.97\n",
            "Round  79: train_corr=+0.0588 valid_corr=+nan trees/s=64.76\n",
            "Round  80: train_corr=+0.0590 valid_corr=+nan trees/s=59.65\n",
            "Round  81: train_corr=+0.0592 valid_corr=+nan trees/s=53.64\n",
            "Round  82: train_corr=+0.0594 valid_corr=+nan trees/s=57.83\n",
            "Round  83: train_corr=+0.0597 valid_corr=+nan trees/s=57.65\n",
            "Round  84: train_corr=+0.0599 valid_corr=+nan trees/s=49.68\n",
            "Round  85: train_corr=+0.0599 valid_corr=+nan trees/s=43.53\n",
            "Round  86: train_corr=+0.0602 valid_corr=+nan trees/s=46.46\n",
            "Round  87: train_corr=+0.0604 valid_corr=+nan trees/s=56.89\n",
            "Round  88: train_corr=+0.0604 valid_corr=+nan trees/s=67.05\n",
            "Round  89: train_corr=+0.0606 valid_corr=+nan trees/s=49.17\n",
            "Round  90: train_corr=+0.0608 valid_corr=+nan trees/s=72.21\n",
            "Round  91: train_corr=+0.0610 valid_corr=+nan trees/s=50.32\n",
            "Round  92: train_corr=+0.0612 valid_corr=+nan trees/s=53.38\n",
            "Round  93: train_corr=+0.0612 valid_corr=+nan trees/s=54.74\n",
            "Round  94: train_corr=+0.0615 valid_corr=+nan trees/s=51.67\n",
            "Round  95: train_corr=+0.0618 valid_corr=+nan trees/s=59.47\n",
            "Round  96: train_corr=+0.0620 valid_corr=+nan trees/s=64.61\n",
            "Round  97: train_corr=+0.0621 valid_corr=+nan trees/s=56.02\n",
            "Round  98: train_corr=+0.0622 valid_corr=+nan trees/s=53.27\n",
            "Round  99: train_corr=+0.0623 valid_corr=+nan trees/s=58.93\n",
            "Round 100: train_corr=+0.0623 valid_corr=+0.0190 trees/s=6.50\n",
            "Round 101: train_corr=+0.0625 valid_corr=+0.0190 trees/s=59.17\n",
            "Round 102: train_corr=+0.0627 valid_corr=+0.0190 trees/s=62.82\n",
            "Round 103: train_corr=+0.0628 valid_corr=+0.0190 trees/s=64.91\n",
            "Round 104: train_corr=+0.0626 valid_corr=+0.0190 trees/s=61.26\n",
            "Round 105: train_corr=+0.0626 valid_corr=+0.0190 trees/s=56.21\n",
            "Round 106: train_corr=+0.0628 valid_corr=+0.0190 trees/s=42.13\n",
            "Round 107: train_corr=+0.0631 valid_corr=+0.0190 trees/s=45.68\n",
            "Round 108: train_corr=+0.0633 valid_corr=+0.0190 trees/s=50.02\n",
            "Round 109: train_corr=+0.0635 valid_corr=+0.0190 trees/s=62.97\n",
            "Round 110: train_corr=+0.0636 valid_corr=+0.0190 trees/s=50.30\n",
            "Round 111: train_corr=+0.0637 valid_corr=+0.0190 trees/s=46.46\n",
            "Round 112: train_corr=+0.0639 valid_corr=+0.0190 trees/s=58.09\n",
            "Round 113: train_corr=+0.0641 valid_corr=+0.0190 trees/s=48.63\n",
            "Round 114: train_corr=+0.0642 valid_corr=+0.0190 trees/s=71.16\n",
            "Round 115: train_corr=+0.0644 valid_corr=+0.0190 trees/s=47.57\n",
            "Round 116: train_corr=+0.0644 valid_corr=+0.0190 trees/s=60.99\n",
            "Round 117: train_corr=+0.0643 valid_corr=+0.0190 trees/s=54.62\n",
            "Round 118: train_corr=+0.0643 valid_corr=+0.0190 trees/s=64.42\n",
            "Round 119: train_corr=+0.0644 valid_corr=+0.0190 trees/s=61.86\n",
            "Round 120: train_corr=+0.0646 valid_corr=+0.0190 trees/s=50.96\n",
            "Round 121: train_corr=+0.0646 valid_corr=+0.0190 trees/s=47.46\n",
            "Round 122: train_corr=+0.0647 valid_corr=+0.0190 trees/s=58.73\n",
            "Round 123: train_corr=+0.0649 valid_corr=+0.0190 trees/s=51.34\n",
            "Round 124: train_corr=+0.0650 valid_corr=+0.0190 trees/s=61.46\n",
            "Round 125: train_corr=+0.0651 valid_corr=+0.0190 trees/s=61.62\n",
            "Round 126: train_corr=+0.0652 valid_corr=+0.0190 trees/s=55.75\n",
            "Round 127: train_corr=+0.0653 valid_corr=+0.0190 trees/s=67.44\n",
            "Round 128: train_corr=+0.0656 valid_corr=+0.0190 trees/s=60.23\n",
            "Round 129: train_corr=+0.0658 valid_corr=+0.0190 trees/s=57.26\n",
            "Round 130: train_corr=+0.0659 valid_corr=+0.0190 trees/s=61.94\n",
            "Round 131: train_corr=+0.0660 valid_corr=+0.0190 trees/s=53.25\n",
            "Round 132: train_corr=+0.0663 valid_corr=+0.0190 trees/s=58.55\n",
            "Round 133: train_corr=+0.0665 valid_corr=+0.0190 trees/s=42.99\n",
            "Round 134: train_corr=+0.0667 valid_corr=+0.0190 trees/s=60.14\n",
            "Round 135: train_corr=+0.0667 valid_corr=+0.0190 trees/s=63.20\n",
            "Round 136: train_corr=+0.0668 valid_corr=+0.0190 trees/s=58.04\n",
            "Round 137: train_corr=+0.0671 valid_corr=+0.0190 trees/s=46.46\n",
            "Round 138: train_corr=+0.0671 valid_corr=+0.0190 trees/s=59.25\n",
            "Round 139: train_corr=+0.0671 valid_corr=+0.0190 trees/s=66.21\n",
            "Round 140: train_corr=+0.0673 valid_corr=+0.0190 trees/s=63.48\n",
            "Round 141: train_corr=+0.0674 valid_corr=+0.0190 trees/s=57.73\n",
            "Round 142: train_corr=+0.0675 valid_corr=+0.0190 trees/s=56.60\n",
            "Round 143: train_corr=+0.0676 valid_corr=+0.0190 trees/s=71.41\n",
            "Round 144: train_corr=+0.0678 valid_corr=+0.0190 trees/s=41.57\n",
            "Round 145: train_corr=+0.0681 valid_corr=+0.0190 trees/s=54.30\n",
            "Round 146: train_corr=+0.0682 valid_corr=+0.0190 trees/s=53.38\n",
            "Round 147: train_corr=+0.0681 valid_corr=+0.0190 trees/s=59.46\n",
            "Round 148: train_corr=+0.0683 valid_corr=+0.0190 trees/s=62.70\n",
            "Round 149: train_corr=+0.0684 valid_corr=+0.0190 trees/s=48.37\n",
            "Round 150: train_corr=+0.0686 valid_corr=+0.0190 trees/s=43.72\n",
            "Round 151: train_corr=+0.0688 valid_corr=+0.0190 trees/s=47.65\n",
            "Round 152: train_corr=+0.0689 valid_corr=+0.0190 trees/s=46.34\n",
            "Round 153: train_corr=+0.0690 valid_corr=+0.0190 trees/s=64.35\n",
            "Round 154: train_corr=+0.0690 valid_corr=+0.0190 trees/s=57.34\n",
            "Round 155: train_corr=+0.0691 valid_corr=+0.0190 trees/s=70.75\n",
            "Round 156: train_corr=+0.0693 valid_corr=+0.0190 trees/s=55.44\n",
            "Round 157: train_corr=+0.0694 valid_corr=+0.0190 trees/s=62.00\n",
            "Round 158: train_corr=+0.0696 valid_corr=+0.0190 trees/s=61.65\n",
            "Round 159: train_corr=+0.0697 valid_corr=+0.0190 trees/s=56.35\n",
            "Round 160: train_corr=+0.0698 valid_corr=+0.0190 trees/s=57.20\n",
            "Round 161: train_corr=+0.0700 valid_corr=+0.0190 trees/s=51.09\n",
            "Round 162: train_corr=+0.0701 valid_corr=+0.0190 trees/s=55.46\n",
            "Round 163: train_corr=+0.0703 valid_corr=+0.0190 trees/s=55.72\n",
            "Round 164: train_corr=+0.0703 valid_corr=+0.0190 trees/s=69.19\n",
            "Round 165: train_corr=+0.0705 valid_corr=+0.0190 trees/s=57.72\n",
            "Round 166: train_corr=+0.0707 valid_corr=+0.0190 trees/s=47.40\n",
            "Round 167: train_corr=+0.0708 valid_corr=+0.0190 trees/s=49.05\n",
            "Round 168: train_corr=+0.0711 valid_corr=+0.0190 trees/s=45.67\n",
            "Round 169: train_corr=+0.0711 valid_corr=+0.0190 trees/s=51.32\n",
            "Round 170: train_corr=+0.0712 valid_corr=+0.0190 trees/s=72.16\n",
            "Round 171: train_corr=+0.0713 valid_corr=+0.0190 trees/s=54.42\n",
            "Round 172: train_corr=+0.0714 valid_corr=+0.0190 trees/s=63.67\n",
            "Round 173: train_corr=+0.0715 valid_corr=+0.0190 trees/s=48.24\n",
            "Round 174: train_corr=+0.0716 valid_corr=+0.0190 trees/s=52.22\n",
            "Round 175: train_corr=+0.0718 valid_corr=+0.0190 trees/s=48.80\n",
            "Round 176: train_corr=+0.0718 valid_corr=+0.0190 trees/s=67.29\n",
            "Round 177: train_corr=+0.0719 valid_corr=+0.0190 trees/s=57.97\n",
            "Round 178: train_corr=+0.0720 valid_corr=+0.0190 trees/s=57.99\n",
            "Round 179: train_corr=+0.0720 valid_corr=+0.0190 trees/s=65.11\n",
            "Round 180: train_corr=+0.0722 valid_corr=+0.0190 trees/s=68.63\n",
            "Round 181: train_corr=+0.0723 valid_corr=+0.0190 trees/s=46.46\n",
            "Round 182: train_corr=+0.0724 valid_corr=+0.0190 trees/s=56.65\n",
            "Round 183: train_corr=+0.0724 valid_corr=+0.0190 trees/s=55.98\n",
            "Round 184: train_corr=+0.0726 valid_corr=+0.0190 trees/s=51.49\n",
            "Round 185: train_corr=+0.0726 valid_corr=+0.0190 trees/s=54.71\n",
            "Round 186: train_corr=+0.0727 valid_corr=+0.0190 trees/s=66.31\n",
            "Round 187: train_corr=+0.0727 valid_corr=+0.0190 trees/s=56.30\n",
            "Round 188: train_corr=+0.0728 valid_corr=+0.0190 trees/s=53.51\n",
            "Round 189: train_corr=+0.0731 valid_corr=+0.0190 trees/s=64.11\n",
            "Round 190: train_corr=+0.0732 valid_corr=+0.0190 trees/s=48.56\n",
            "Round 191: train_corr=+0.0733 valid_corr=+0.0190 trees/s=63.44\n",
            "Round 192: train_corr=+0.0733 valid_corr=+0.0190 trees/s=47.46\n",
            "Round 193: train_corr=+0.0734 valid_corr=+0.0190 trees/s=51.21\n",
            "Round 194: train_corr=+0.0735 valid_corr=+0.0190 trees/s=52.54\n",
            "Round 195: train_corr=+0.0736 valid_corr=+0.0190 trees/s=48.73\n",
            "Round 196: train_corr=+0.0735 valid_corr=+0.0190 trees/s=48.91\n",
            "Round 197: train_corr=+0.0737 valid_corr=+0.0190 trees/s=52.94\n",
            "Round 198: train_corr=+0.0738 valid_corr=+0.0190 trees/s=59.00\n",
            "Round 199: train_corr=+0.0739 valid_corr=+0.0190 trees/s=51.04\n",
            "Round 200: train_corr=+0.0740 valid_corr=+0.0212 trees/s=3.37\n",
            "Round 201: train_corr=+0.0743 valid_corr=+0.0212 trees/s=56.09\n",
            "Round 202: train_corr=+0.0744 valid_corr=+0.0212 trees/s=60.71\n",
            "Round 203: train_corr=+0.0745 valid_corr=+0.0212 trees/s=61.08\n",
            "Round 204: train_corr=+0.0745 valid_corr=+0.0212 trees/s=63.05\n",
            "Round 205: train_corr=+0.0745 valid_corr=+0.0212 trees/s=57.09\n",
            "Round 206: train_corr=+0.0748 valid_corr=+0.0212 trees/s=52.42\n",
            "Round 207: train_corr=+0.0750 valid_corr=+0.0212 trees/s=54.73\n",
            "Round 208: train_corr=+0.0751 valid_corr=+0.0212 trees/s=42.19\n",
            "Round 209: train_corr=+0.0753 valid_corr=+0.0212 trees/s=65.81\n",
            "Round 210: train_corr=+0.0754 valid_corr=+0.0212 trees/s=61.27\n",
            "Round 211: train_corr=+0.0755 valid_corr=+0.0212 trees/s=48.60\n",
            "Round 212: train_corr=+0.0756 valid_corr=+0.0212 trees/s=60.29\n",
            "Round 213: train_corr=+0.0757 valid_corr=+0.0212 trees/s=58.38\n",
            "Round 214: train_corr=+0.0757 valid_corr=+0.0212 trees/s=59.67\n",
            "Round 215: train_corr=+0.0758 valid_corr=+0.0212 trees/s=56.76\n",
            "Round 216: train_corr=+0.0761 valid_corr=+0.0212 trees/s=57.45\n",
            "Round 217: train_corr=+0.0763 valid_corr=+0.0212 trees/s=55.05\n",
            "Round 218: train_corr=+0.0763 valid_corr=+0.0212 trees/s=52.69\n",
            "Round 219: train_corr=+0.0765 valid_corr=+0.0212 trees/s=49.76\n",
            "Round 220: train_corr=+0.0766 valid_corr=+0.0212 trees/s=57.72\n",
            "Round 221: train_corr=+0.0767 valid_corr=+0.0212 trees/s=56.63\n",
            "Round 222: train_corr=+0.0768 valid_corr=+0.0212 trees/s=64.81\n",
            "Round 223: train_corr=+0.0768 valid_corr=+0.0212 trees/s=51.68\n",
            "Round 224: train_corr=+0.0770 valid_corr=+0.0212 trees/s=42.98\n",
            "Round 225: train_corr=+0.0771 valid_corr=+0.0212 trees/s=47.76\n",
            "Round 226: train_corr=+0.0772 valid_corr=+0.0212 trees/s=61.39\n",
            "Round 227: train_corr=+0.0772 valid_corr=+0.0212 trees/s=64.93\n",
            "Round 228: train_corr=+0.0772 valid_corr=+0.0212 trees/s=61.23\n",
            "Round 229: train_corr=+0.0773 valid_corr=+0.0212 trees/s=66.26\n",
            "Round 230: train_corr=+0.0774 valid_corr=+0.0212 trees/s=63.05\n",
            "Round 231: train_corr=+0.0774 valid_corr=+0.0212 trees/s=75.29\n",
            "Round 232: train_corr=+0.0776 valid_corr=+0.0212 trees/s=61.14\n",
            "Round 233: train_corr=+0.0777 valid_corr=+0.0212 trees/s=70.30\n",
            "Round 234: train_corr=+0.0779 valid_corr=+0.0212 trees/s=66.33\n",
            "Round 235: train_corr=+0.0779 valid_corr=+0.0212 trees/s=66.85\n",
            "Round 236: train_corr=+0.0780 valid_corr=+0.0212 trees/s=59.28\n",
            "Round 237: train_corr=+0.0781 valid_corr=+0.0212 trees/s=45.55\n",
            "Round 238: train_corr=+0.0782 valid_corr=+0.0212 trees/s=68.47\n",
            "Round 239: train_corr=+0.0783 valid_corr=+0.0212 trees/s=49.11\n",
            "Round 240: train_corr=+0.0785 valid_corr=+0.0212 trees/s=66.58\n",
            "Round 241: train_corr=+0.0785 valid_corr=+0.0212 trees/s=52.90\n",
            "Round 242: train_corr=+0.0787 valid_corr=+0.0212 trees/s=54.60\n",
            "Round 243: train_corr=+0.0787 valid_corr=+0.0212 trees/s=52.68\n",
            "Round 244: train_corr=+0.0788 valid_corr=+0.0212 trees/s=65.14\n",
            "Round 245: train_corr=+0.0789 valid_corr=+0.0212 trees/s=54.56\n",
            "Round 246: train_corr=+0.0790 valid_corr=+0.0212 trees/s=66.92\n",
            "Round 247: train_corr=+0.0791 valid_corr=+0.0212 trees/s=56.10\n",
            "Round 248: train_corr=+0.0792 valid_corr=+0.0212 trees/s=67.41\n",
            "Round 249: train_corr=+0.0793 valid_corr=+0.0212 trees/s=60.50\n",
            "Round 250: train_corr=+0.0793 valid_corr=+0.0212 trees/s=52.29\n",
            "Round 251: train_corr=+0.0795 valid_corr=+0.0212 trees/s=65.26\n",
            "Round 252: train_corr=+0.0796 valid_corr=+0.0212 trees/s=63.32\n",
            "Round 253: train_corr=+0.0797 valid_corr=+0.0212 trees/s=50.49\n",
            "Round 254: train_corr=+0.0797 valid_corr=+0.0212 trees/s=70.47\n",
            "Round 255: train_corr=+0.0798 valid_corr=+0.0212 trees/s=65.98\n",
            "Round 256: train_corr=+0.0798 valid_corr=+0.0212 trees/s=49.55\n",
            "Round 257: train_corr=+0.0799 valid_corr=+0.0212 trees/s=48.86\n",
            "Round 258: train_corr=+0.0799 valid_corr=+0.0212 trees/s=54.05\n",
            "Round 259: train_corr=+0.0800 valid_corr=+0.0212 trees/s=54.05\n",
            "Round 260: train_corr=+0.0802 valid_corr=+0.0212 trees/s=62.08\n",
            "Round 261: train_corr=+0.0803 valid_corr=+0.0212 trees/s=44.88\n",
            "Round 262: train_corr=+0.0804 valid_corr=+0.0212 trees/s=64.65\n",
            "Round 263: train_corr=+0.0805 valid_corr=+0.0212 trees/s=67.78\n",
            "Round 264: train_corr=+0.0807 valid_corr=+0.0212 trees/s=52.61\n",
            "Round 265: train_corr=+0.0807 valid_corr=+0.0212 trees/s=43.56\n",
            "Round 266: train_corr=+0.0808 valid_corr=+0.0212 trees/s=63.35\n",
            "Round 267: train_corr=+0.0809 valid_corr=+0.0212 trees/s=54.18\n",
            "Round 268: train_corr=+0.0810 valid_corr=+0.0212 trees/s=64.17\n",
            "Round 269: train_corr=+0.0812 valid_corr=+0.0212 trees/s=58.70\n",
            "Round 270: train_corr=+0.0812 valid_corr=+0.0212 trees/s=55.48\n",
            "Round 271: train_corr=+0.0814 valid_corr=+0.0212 trees/s=43.49\n",
            "Round 272: train_corr=+0.0814 valid_corr=+0.0212 trees/s=47.39\n",
            "Round 273: train_corr=+0.0814 valid_corr=+0.0212 trees/s=50.45\n",
            "Round 274: train_corr=+0.0816 valid_corr=+0.0212 trees/s=62.29\n",
            "Round 275: train_corr=+0.0816 valid_corr=+0.0212 trees/s=44.41\n",
            "Round 276: train_corr=+0.0817 valid_corr=+0.0212 trees/s=55.48\n",
            "Round 277: train_corr=+0.0819 valid_corr=+0.0212 trees/s=67.58\n",
            "Round 278: train_corr=+0.0820 valid_corr=+0.0212 trees/s=52.01\n",
            "Round 279: train_corr=+0.0820 valid_corr=+0.0212 trees/s=46.50\n",
            "Round 280: train_corr=+0.0820 valid_corr=+0.0212 trees/s=60.66\n",
            "Round 281: train_corr=+0.0821 valid_corr=+0.0212 trees/s=50.53\n",
            "Round 282: train_corr=+0.0822 valid_corr=+0.0212 trees/s=57.58\n",
            "Round 283: train_corr=+0.0823 valid_corr=+0.0212 trees/s=60.19\n",
            "Round 284: train_corr=+0.0824 valid_corr=+0.0212 trees/s=59.74\n",
            "Round 285: train_corr=+0.0825 valid_corr=+0.0212 trees/s=59.95\n",
            "Round 286: train_corr=+0.0826 valid_corr=+0.0212 trees/s=61.04\n",
            "Round 287: train_corr=+0.0827 valid_corr=+0.0212 trees/s=66.96\n",
            "Round 288: train_corr=+0.0827 valid_corr=+0.0212 trees/s=58.05\n",
            "Round 289: train_corr=+0.0827 valid_corr=+0.0212 trees/s=69.02\n",
            "Round 290: train_corr=+0.0828 valid_corr=+0.0212 trees/s=53.95\n",
            "Round 291: train_corr=+0.0829 valid_corr=+0.0212 trees/s=54.68\n",
            "Round 292: train_corr=+0.0831 valid_corr=+0.0212 trees/s=55.55\n",
            "Round 293: train_corr=+0.0831 valid_corr=+0.0212 trees/s=54.60\n",
            "Round 294: train_corr=+0.0833 valid_corr=+0.0212 trees/s=60.22\n",
            "Round 295: train_corr=+0.0834 valid_corr=+0.0212 trees/s=64.64\n",
            "Round 296: train_corr=+0.0835 valid_corr=+0.0212 trees/s=42.80\n",
            "Round 297: train_corr=+0.0837 valid_corr=+0.0212 trees/s=59.07\n",
            "Round 298: train_corr=+0.0837 valid_corr=+0.0212 trees/s=66.16\n",
            "Round 299: train_corr=+0.0837 valid_corr=+0.0212 trees/s=62.31\n",
            "Round 300: train_corr=+0.0838 valid_corr=+0.0229 trees/s=2.31\n",
            "Round 301: train_corr=+0.0839 valid_corr=+0.0229 trees/s=64.01\n",
            "Round 302: train_corr=+0.0841 valid_corr=+0.0229 trees/s=23.81\n",
            "Round 303: train_corr=+0.0841 valid_corr=+0.0229 trees/s=64.83\n",
            "Round 304: train_corr=+0.0842 valid_corr=+0.0229 trees/s=52.58\n",
            "Round 305: train_corr=+0.0844 valid_corr=+0.0229 trees/s=55.28\n",
            "Round 306: train_corr=+0.0844 valid_corr=+0.0229 trees/s=61.51\n",
            "Round 307: train_corr=+0.0845 valid_corr=+0.0229 trees/s=65.77\n",
            "Round 308: train_corr=+0.0846 valid_corr=+0.0229 trees/s=50.66\n",
            "Round 309: train_corr=+0.0847 valid_corr=+0.0229 trees/s=47.94\n",
            "Round 310: train_corr=+0.0848 valid_corr=+0.0229 trees/s=66.51\n",
            "Round 311: train_corr=+0.0849 valid_corr=+0.0229 trees/s=59.85\n",
            "Round 312: train_corr=+0.0850 valid_corr=+0.0229 trees/s=58.28\n",
            "Round 313: train_corr=+0.0851 valid_corr=+0.0229 trees/s=56.47\n",
            "Round 314: train_corr=+0.0852 valid_corr=+0.0229 trees/s=61.04\n",
            "Round 315: train_corr=+0.0852 valid_corr=+0.0229 trees/s=52.79\n",
            "Round 316: train_corr=+0.0853 valid_corr=+0.0229 trees/s=56.22\n",
            "Round 317: train_corr=+0.0854 valid_corr=+0.0229 trees/s=46.14\n",
            "Round 318: train_corr=+0.0855 valid_corr=+0.0229 trees/s=53.14\n",
            "Round 319: train_corr=+0.0855 valid_corr=+0.0229 trees/s=59.98\n",
            "Round 320: train_corr=+0.0856 valid_corr=+0.0229 trees/s=55.46\n",
            "Round 321: train_corr=+0.0857 valid_corr=+0.0229 trees/s=54.09\n",
            "Round 322: train_corr=+0.0858 valid_corr=+0.0229 trees/s=63.74\n",
            "Round 323: train_corr=+0.0859 valid_corr=+0.0229 trees/s=55.68\n",
            "Round 324: train_corr=+0.0860 valid_corr=+0.0229 trees/s=44.85\n",
            "Round 325: train_corr=+0.0861 valid_corr=+0.0229 trees/s=48.05\n",
            "Round 326: train_corr=+0.0861 valid_corr=+0.0229 trees/s=64.96\n",
            "Round 327: train_corr=+0.0862 valid_corr=+0.0229 trees/s=46.32\n",
            "Round 328: train_corr=+0.0862 valid_corr=+0.0229 trees/s=57.67\n",
            "Round 329: train_corr=+0.0863 valid_corr=+0.0229 trees/s=65.51\n",
            "Round 330: train_corr=+0.0864 valid_corr=+0.0229 trees/s=71.25\n",
            "Round 331: train_corr=+0.0866 valid_corr=+0.0229 trees/s=43.80\n",
            "Round 332: train_corr=+0.0866 valid_corr=+0.0229 trees/s=63.25\n",
            "Round 333: train_corr=+0.0867 valid_corr=+0.0229 trees/s=51.59\n",
            "Round 334: train_corr=+0.0868 valid_corr=+0.0229 trees/s=64.28\n",
            "Round 335: train_corr=+0.0869 valid_corr=+0.0229 trees/s=59.72\n",
            "Round 336: train_corr=+0.0870 valid_corr=+0.0229 trees/s=60.28\n",
            "Round 337: train_corr=+0.0870 valid_corr=+0.0229 trees/s=54.22\n",
            "Round 338: train_corr=+0.0871 valid_corr=+0.0229 trees/s=58.39\n",
            "Round 339: train_corr=+0.0871 valid_corr=+0.0229 trees/s=60.60\n",
            "Round 340: train_corr=+0.0872 valid_corr=+0.0229 trees/s=67.83\n",
            "Round 341: train_corr=+0.0873 valid_corr=+0.0229 trees/s=55.92\n",
            "Round 342: train_corr=+0.0875 valid_corr=+0.0229 trees/s=54.31\n",
            "Round 343: train_corr=+0.0877 valid_corr=+0.0229 trees/s=49.60\n",
            "Round 344: train_corr=+0.0878 valid_corr=+0.0229 trees/s=49.48\n",
            "Round 345: train_corr=+0.0880 valid_corr=+0.0229 trees/s=44.65\n",
            "Round 346: train_corr=+0.0881 valid_corr=+0.0229 trees/s=62.66\n",
            "Round 347: train_corr=+0.0882 valid_corr=+0.0229 trees/s=47.77\n",
            "Round 348: train_corr=+0.0882 valid_corr=+0.0229 trees/s=52.13\n",
            "Round 349: train_corr=+0.0884 valid_corr=+0.0229 trees/s=51.27\n",
            "Round 350: train_corr=+0.0885 valid_corr=+0.0229 trees/s=62.44\n",
            "Round 351: train_corr=+0.0887 valid_corr=+0.0229 trees/s=48.13\n",
            "Round 352: train_corr=+0.0887 valid_corr=+0.0229 trees/s=41.72\n",
            "Round 353: train_corr=+0.0889 valid_corr=+0.0229 trees/s=39.94\n",
            "Round 354: train_corr=+0.0891 valid_corr=+0.0229 trees/s=55.69\n",
            "Round 355: train_corr=+0.0891 valid_corr=+0.0229 trees/s=60.96\n",
            "Round 356: train_corr=+0.0892 valid_corr=+0.0229 trees/s=58.97\n",
            "Round 357: train_corr=+0.0892 valid_corr=+0.0229 trees/s=44.60\n",
            "Round 358: train_corr=+0.0893 valid_corr=+0.0229 trees/s=49.76\n",
            "Round 359: train_corr=+0.0894 valid_corr=+0.0229 trees/s=68.49\n",
            "Round 360: train_corr=+0.0894 valid_corr=+0.0229 trees/s=54.45\n",
            "Round 361: train_corr=+0.0895 valid_corr=+0.0229 trees/s=71.36\n",
            "Round 362: train_corr=+0.0895 valid_corr=+0.0229 trees/s=67.73\n",
            "Round 363: train_corr=+0.0896 valid_corr=+0.0229 trees/s=53.89\n",
            "Round 364: train_corr=+0.0898 valid_corr=+0.0229 trees/s=44.05\n",
            "Round 365: train_corr=+0.0898 valid_corr=+0.0229 trees/s=57.16\n"
          ]
        }
      ],
      "source": [
        "from packboost.booster import PackBoost\n",
        "from packboost.config import PackBoostConfig\n",
        "from packboost import backends\n",
        "from time import perf_counter\n",
        "import pandas as pd\n",
        "\n",
        "print('CUDA backend available:', backends.cuda_available())\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "config = PackBoostConfig(\n",
        "    pack_size=8,\n",
        "    max_depth=7,\n",
        "    learning_rate=0.07,\n",
        "    lambda_l2=1e-6,\n",
        "    lambda_dro=0.0,\n",
        "    direction_weight=0.0,\n",
        "    min_samples_leaf=20,\n",
        "    max_bins=5,\n",
        "    k_cuts=4,\n",
        "    cut_selection='even',\n",
        "    device=str(DEVICE),\n",
        "    prebinned=True,\n",
        "    layer_feature_fraction=0.0538,\n",
        "    random_state=42,\n",
        "    histogram_mode='subtract'\n",
        ")\n",
        "\n",
        "round_logs = []\n",
        "\n",
        "def log_round(idx: int, metrics: dict[str, float]) -> None:\n",
        "    print(f\"Round {metrics['round']:>3}: train_corr={metrics['train_corr']:+.4f} \"\n",
        "          f\"valid_corr={metrics.get('valid_corr', float('nan')):+.4f} \"\n",
        "          f\"trees/s={metrics['trees_per_second']:.2f}\")\n",
        "    round_logs.append(metrics)\n",
        "\n",
        "booster = PackBoost(config)\n",
        "start = perf_counter()\n",
        "booster.fit(\n",
        "    Xt,\n",
        "    yt,\n",
        "    Et, #Et\n",
        "    num_rounds=5000,\n",
        "    eval_sets=[('valid', Xv, Yv, None)],\n",
        "    round_callback=log_round,\n",
        ")\n",
        "elapsed = perf_counter() - start\n",
        "print(f\"Finished training in {elapsed:.2f} seconds\")\n",
        "\n",
        "metrics_df = pd.DataFrame(round_logs)\n",
        "metrics_df\n"
      ],
      "id": "train"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKddr1wZpxy9"
      },
      "source": [
        "## Plot per-round correlation\n"
      ],
      "id": "XKddr1wZpxy9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "if not metrics_df.empty:\n",
        "    ax = metrics_df.plot(x='round', y=['train_corr', 'valid_corr'], marker='o')\n",
        "    ax.set_ylabel('Correlation')\n",
        "    ax.set_title('PackBoost correlation per round')\n",
        "    ax.grid(True)\n",
        "    plt.show()\n"
      ],
      "id": "plot"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDY8fVd0pxy9"
      },
      "source": [
        "## Generate validation predictions\n",
        "\n",
        "Predictions are normalised to the [0,1] range expected by Numerai diagnostics.\n"
      ],
      "id": "VDY8fVd0pxy9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "predict"
      },
      "outputs": [],
      "source": [
        "pred_valid = booster.predict(Xv)\n",
        "\n",
        "pred_norm = pred_valid.copy()\n",
        "pred_norm -= pred_norm.min()\n",
        "if pred_norm.max() > 0:\n",
        "    pred_norm /= pred_norm.max()\n",
        "pred_norm = np.clip(pred_norm * 0.98 + 0.01, 0.0, 1.0)\n",
        "\n",
        "submission = pd.DataFrame({'prediction': pred_norm})\n",
        "submission.to_csv('packboost_predictions.csv', index=False)\n",
        "print('Saved packboost_predictions.csv')\n",
        "\n",
        "gc.collect()\n"
      ],
      "id": "predict"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}