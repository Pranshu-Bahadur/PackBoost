{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PackBoost Numerai GPU Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: install dependencies in fresh environments\n",
        "# !pip install -q numerapi\n",
        "\n",
        "import gc\n",
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from numerapi import NumerAPI\n",
        "\n",
        "from packboost.booster import PackBoost\n",
        "from packboost.config import PackBoostConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numerai credentials (only needed for diagnostics upload)\n",
        "your_public_id = \"\"\n",
        "your_secret_key = \"\"\n",
        "your_model_slot_name = \"\"\n",
        "\n",
        "# Training hyperparameters\n",
        "ERA_BUCKET_SIZE = 64        # consecutive eras per bucket for DES stability\n",
        "NUM_ROUNDS = 20             # number of boosting rounds\n",
        "PACK_SIZE = 8               # trees grown per round\n",
        "MAX_DEPTH = 6\n",
        "LEARNING_RATE = 0.05\n",
        "MIN_SAMPLES_LEAF = 20\n",
        "MAX_BINS = 64\n",
        "K_CUTS = 0                  # 0 => use all thresholds\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DATA_VERSION = \"v5.0\"\n",
        "DATA_DIR = Path(DATA_VERSION)\n",
        "DATA_DIR.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Download Numerai data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "napi = NumerAPI(your_public_id, your_secret_key)\n",
        "\n",
        "feature_path = DATA_DIR / \"features.json\"\n",
        "train_path = DATA_DIR / \"train.parquet\"\n",
        "valid_path = DATA_DIR / \"validation.parquet\"\n",
        "\n",
        "if not feature_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/features.json\", str(feature_path))\n",
        "if not train_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/train.parquet\", str(train_path))\n",
        "if not valid_path.exists():\n",
        "    napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\", str(valid_path))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with feature_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
        "    features_meta: Dict = json.load(fh)\n",
        "FEATURES: List[str] = features_meta[\"feature_sets\"][\"all\"]\n",
        "\n",
        "train_df = pd.read_parquet(train_path)\n",
        "train_df[\"era\"] = train_df[\"era\"].astype(np.int32)\n",
        "train_df = train_df.dropna(subset=[\"target\"]).reset_index(drop=True)\n",
        "train_df[\"era_bucket\"] = (train_df[\"era\"] // ERA_BUCKET_SIZE).astype(np.int32)\n",
        "\n",
        "Xt = train_df[FEATURES].fillna(2).astype(np.uint8).values\n",
        "yt = train_df[\"target\"].astype(np.float32).values\n",
        "Et = train_df[\"era_bucket\"].to_numpy(np.int32)\n",
        "\n",
        "del train_df\n",
        "gc.collect()\n",
        "\n",
        "valid_df = pd.read_parquet(valid_path)\n",
        "valid_df = valid_df.dropna(subset=[\"target\"])                     .reset_index(drop=True)\n",
        "valid_df[\"era\"] = valid_df[\"era\"].astype(np.int32)\n",
        "valid_df[\"era_bucket\"] = (valid_df[\"era\"] // ERA_BUCKET_SIZE).astype(np.int32)\n",
        "\n",
        "Xv = valid_df[FEATURES].fillna(2).astype(np.uint8).values\n",
        "Yv = valid_df[\"target\"].astype(np.float32).values\n",
        "Ev = valid_df[\"era_bucket\"].to_numpy(np.int32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train PackBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = PackBoostConfig(\n",
        "    pack_size=PACK_SIZE,\n",
        "    max_depth=MAX_DEPTH,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    lambda_l2=1e-6,\n",
        "    lambda_dro=0.0,\n",
        "    direction_weight=0.0,\n",
        "    min_samples_leaf=MIN_SAMPLES_LEAF,\n",
        "    max_bins=MAX_BINS,\n",
        "    k_cuts=K_CUTS,\n",
        "    device=str(DEVICE),\n",
        "    prebinned=True,\n",
        ")\n",
        "\n",
        "booster = PackBoost(config)\n",
        "booster.fit(Xt, yt, Et, num_rounds=NUM_ROUNDS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Per-round train/validation correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def era_correlation(eras: np.ndarray, target: np.ndarray, pred: np.ndarray) -> float:\n",
        "    frame = pd.DataFrame({\"era\": eras, \"target\": target, \"pred\": pred})\n",
        "    cors = []\n",
        "    for _, grp in frame.groupby(\"era\", sort=False):\n",
        "        if grp[\"target\"].std(ddof=0) == 0 or grp[\"pred\"].std(ddof=0) == 0:\n",
        "            continue\n",
        "        val = np.corrcoef(grp[\"target\"], grp[\"pred\"])[0, 1]\n",
        "        if np.isfinite(val):\n",
        "            cors.append(val)\n",
        "    return float(np.mean(cors)) if cors else float(\"nan\")\n",
        "\n",
        "\n",
        "def compute_round_correlations(model: PackBoost) -> pd.DataFrame:\n",
        "    B = model._trained_pack_size or model.config.pack_size\n",
        "    weight = float(model._tree_weight or (model.config.learning_rate / model.config.pack_size))\n",
        "    num_packs = len(model._trees) // B\n",
        "\n",
        "    bins_train = torch.from_numpy(Xt).to(model._device, dtype=torch.int32)\n",
        "    bins_valid = torch.from_numpy(Xv).to(model._device, dtype=torch.int32)\n",
        "    y_train = torch.from_numpy(yt).to(model._device, dtype=torch.float32)\n",
        "    y_valid = torch.from_numpy(Yv).to(model._device, dtype=torch.float32)\n",
        "\n",
        "    preds_train = torch.zeros_like(y_train)\n",
        "    preds_valid = torch.zeros_like(y_valid)\n",
        "\n",
        "    records = []\n",
        "    for pack_idx in range(num_packs):\n",
        "        start = pack_idx * B\n",
        "        end = start + B\n",
        "        for tree in model._trees[start:end]:\n",
        "            preds_train += weight * tree.predict_bins(bins_train)\n",
        "            preds_valid += weight * tree.predict_bins(bins_valid)\n",
        "\n",
        "        corr_train = era_correlation(Et, y_train.cpu().numpy(), preds_train.cpu().numpy())\n",
        "        corr_valid = era_correlation(Ev, y_valid.cpu().numpy(), preds_valid.cpu().numpy())\n",
        "        records.append({\"round\": pack_idx + 1, \"train_corr\": corr_train, \"valid_corr\": corr_valid})\n",
        "\n",
        "    return pd.DataFrame(records)\n",
        "\n",
        "round_stats = compute_round_correlations(booster)\n",
        "round_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation predictions & submission scaffold"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_valid = booster.predict(Xv)\n",
        "\n",
        "# scale to [0, 1] for Numerai diagnostics\n",
        "def normalize_preds(preds: np.ndarray) -> np.ndarray:\n",
        "    preds = preds.copy()\n",
        "    preds -= preds.min()\n",
        "    if preds.max() > 0:\n",
        "        preds /= preds.max()\n",
        "    return np.clip(preds * 0.98 + 0.01, 0.0, 1.0)\n",
        "\n",
        "valid_df[\"prediction\"] = normalize_preds(pred_valid.astype(np.float32))\n",
        "valid_df[[\"prediction\"]].to_csv(\"packboost_predictions.csv\", index=False)\n",
        "\n",
        "print(\"Saved packboost_predictions.csv (run napi.upload_diagnostics manually if desired).\")\n",
        "\n",
        "gc.collect()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}